{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Libraries we need\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dataloader objects'''\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "         #(0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "#classes = ('plane', 'car', 'bird', 'cat',\n",
    "#          'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''VGG based networks that work well on CIFAR datasets.'''\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            elif x == 'D2':\n",
    "                layers += [nn.Dropout(p=0.2)]\n",
    "            elif x == 'D3':\n",
    "                layers += [nn.Dropout(p=0.3)]\n",
    "            elif x == 'D4':\n",
    "                layers += [nn.Dropout(p=0.4)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.ReLU(inplace=True),\n",
    "                           nn.BatchNorm2d(x)]\n",
    "                in_channels = x\n",
    "        #layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = VGG('VGG11')\n",
    "    x = torch.randn(2,3,32,32)\n",
    "    y = net(x)\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Setting up the GPU'''\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cal_acc(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cal_acc_train(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 50000 train images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define layer for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_index = 2\n",
    "layer_id = 'bn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline network analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VGG('VGG13').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './corr/cifar100_net.pth'\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_base = net.features[l_index].weight.data.clone().detach()\n",
    "bias_base = net.features[l_index].bias.data.clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth calculation for importance in baseline network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_base_corr = 0\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_base_corr += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mat = torch.zeros(weight_base.shape[0])\n",
    "\n",
    "for n_index in range(weight_base.shape[0]): \n",
    "    print(n_index)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    net.features[l_index].weight.data[n_index] = 0\n",
    "    net.features[l_index].bias.data[n_index] = 0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = (criterion(outputs, labels))\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    loss_mat[n_index] = running_loss\n",
    "    \n",
    "    net.features[l_index].weight.data = weight_base.clone().detach()\n",
    "    net.features[l_index].bias.data = bias_base.clone().detach()\n",
    "\n",
    "torch.save(loss_mat, './corr/loss_bn_'+str(l_index)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_mat = torch.load('./corr/loss_bn_'+str(l_index)+'.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taylor FO calculation for importance in baseline network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0, weight_decay=0)\n",
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    imp_corr_conv = torch.zeros(bias_base.shape[0]).to(device)\n",
    "    imp_corr_bn = torch.zeros(bias_base.shape[0]).to(device)\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        imp_corr_bn += (((net.features[l_index].weight.grad)*(net.features[l_index].weight.data)) + ((net.features[l_index].bias.grad)*(net.features[l_index].bias.data))).abs().pow(2)\n",
    "        \n",
    "    corrval = (np.corrcoef(imp_corr_bn.cpu().detach().numpy(), (loss_mat - loss_base_corr).abs().cpu().detach().numpy()))\n",
    "    print(\"Correlation at epoch \"+str(epoch)+\": \"+str(corrval[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decorrelated network analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './decorr/cifar100_decorrnet.pth'\n",
    "net_decorr = VGG('VGG13').to(device)\n",
    "net_decorr.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_base = net_decorr.features[l_index].weight.data.clone().detach()\n",
    "bias_base = net_decorr.features[l_index].bias.data.clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth calculation for importance in decorrelated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "\n",
    "loss_base_decorr = 0\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net_decorr(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_base_decorr += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "\n",
    "loss_mat_decorr = torch.zeros(weight_base.shape[0])\n",
    "\n",
    "for n_index in range(weight_base.shape[0]): \n",
    "    print(n_index)\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        net_decorr.features[l_index].weight.data[n_index] = 0\n",
    "        net_decorr.features[l_index].bias.data[n_index] = 0 \n",
    "        outputs = net_decorr(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    loss_mat_decorr[n_index] = running_loss\n",
    "    \n",
    "    net_decorr.features[l_index].weight.data = weight_base.clone().detach()\n",
    "    net_decorr.features[l_index].bias.data = bias_base.clone().detach()\n",
    "\n",
    "torch.save(loss_mat_decorr, './decorr/loss_bn_'+str(l_index)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_mat_decorr = torch.load('./decorr/loss_bn_'+str(l_index)+'.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taylor FO calculation for importance in decorrelated network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    imp_decorr_conv = torch.zeros(bias_base.shape[0]).to(device)\n",
    "    imp_decorr_bn = torch.zeros(bias_base.shape[0]).to(device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net_decorr(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        imp_decorr_bn += (((net_decorr.features[l_index].weight.grad)*(net_decorr.features[l_index].weight.data)) + ((net_decorr.features[l_index].bias.grad)*(net_decorr.features[l_index].bias.data))).pow(2)\n",
    "    \n",
    "    corrval = (np.corrcoef(imp_decorr_bn.cpu().detach().numpy(), (loss_mat_decorr - loss_base_decorr).abs().cpu().detach().numpy()))\n",
    "    print(\"Correlation at epoch \"+str(epoch)+\": \"+str(corrval[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net-Slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_corr = net.features[l_index].weight.data.clone()\n",
    "np.corrcoef(scale_corr.cpu().numpy(), (loss_mat - loss_base_corr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_decorr = net_decorr.features[l_index].weight.data.clone().abs()\n",
    "np.corrcoef((scale_decorr).cpu().numpy(), (loss_mat_decorr - loss_base_decorr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_corr = net.features[l_index].weight.data.clone()\n",
    "np.corrcoef(scale_corr.cpu().numpy(), (loss_mat - loss_base_corr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_decorr = net_decorr.features[l_index].weight.data.clone().abs()\n",
    "np.corrcoef((scale_decorr).cpu().numpy(), (loss_mat_decorr - loss_base_decorr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 based pruning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_corr = net.features[l_index - 2].weight.data.clone()\n",
    "w_imp_corr = w_corr.pow(2).sum(dim=(1,2,3)).cpu()\n",
    "np.corrcoef(w_corr.pow(2).sum(dim=(1,2,3)).cpu().numpy(), (loss_mat - loss_base_corr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_decorr = net_decorr.features[l_index - 2].weight.data.clone()\n",
    "w_imp_decorr = w_decorr.pow(2).sum(dim=(1,2,3)).cpu()\n",
    "np.corrcoef(w_decorr.pow(2).sum(dim=(1,2,3)).cpu().numpy(), (loss_mat_decorr - loss_base_decorr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_corr = net.features[l_index - 2].weight.data.clone()\n",
    "w_imp_corr = w_corr.pow(2).sum(dim=(1,2,3)).cpu()\n",
    "np.corrcoef(w_corr.pow(2).sum(dim=(1,2,3)).cpu().numpy(), (loss_mat - loss_base_corr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_decorr = net_decorr.features[l_index - 2].weight.data.clone()\n",
    "w_imp_decorr = w_decorr.pow(2).sum(dim=(1,2,3)).cpu()\n",
    "np.corrcoef(w_decorr.pow(2).sum(dim=(1,2,3)).cpu().numpy(), (loss_mat_decorr - loss_base_decorr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight decorrelated training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net1 = VGG('VGG13').to(device)\n",
    "PATH = './cifar100_net.pth'\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    num_iter = 0\n",
    "    angle_cost = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        num_iter += 1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        L_angle = 0\n",
    "        L_diag = 0\n",
    "        for conv_ind in [0, 3, 7, 10, 14, 17, 21, 24, 28, 31]:\n",
    "            w_mat = net.features[conv_ind].weight\n",
    "            w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "            angle_mat = torch.matmul(w_mat1, torch.t(w_mat1))\n",
    "            L_diag += (w_mat1.pow(2).sum(dim=1)).norm().pow(2) #angle_mat.diag().norm().pow(2) # \n",
    "            L_angle += angle_mat.norm().pow(2)\n",
    "        \n",
    "        Lc = criterion(outputs, labels)\n",
    "        loss = (1e-3)*(L_angle - L_diag) + Lc\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        angle_cost += (L_angle - L_diag).item()\n",
    "    \n",
    "    print(\"angle_cost: \", angle_cost/num_iter)\n",
    "    print(\"diag_mass_ratio: \", L_diag.detach().cpu().numpy()/(L_angle.detach().cpu().numpy()))\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "          (epoch + 1, i + 1, running_loss / num_iter))\n",
    "    running_loss = 0.0\n",
    "    cal_acc(net)\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner product training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  \n",
    "    running_loss = 0.0\n",
    "    cov_loss = 0\n",
    "    num_iter = 0\n",
    "    av_cov_mass = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        num_iter += 1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        for epoch_num in range(1):            \n",
    "            L_cov = 0\n",
    "            for l_index in [3, 6, 10, 13, 17, 20, 24, 27, 31, 34]:\n",
    "                out_features = net.features[0:l_index](inputs)\n",
    "                act_mat = (out_features).permute(0,2,3,1).reshape(-1,out_features.shape[1]) #(0,2,3,1)\n",
    "                cov_mat = torch.matmul(torch.t(act_mat), act_mat) \n",
    "\n",
    "                L_cov += ((cov_mat).norm().pow(2) - cov_mat.diag().norm().pow(2))/2\n",
    "\n",
    "            Lc = criterion(outputs, labels)\n",
    "            loss = Lc + (1e-11)*L_cov\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        cov_loss += L_cov\n",
    "        av_cov_mass += ((cov_mat).diag().norm().cpu().detach().numpy() / cov_mat.norm().cpu().detach().numpy())\n",
    "        \n",
    "        del L_cov, act_mat, cov_mat, out_features\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    print(\"Covariance loss: \" + str(cov_loss/num_iter))\n",
    "    print(av_cov_mass/num_iter)\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "          (epoch + 1, i + 1, running_loss / num_iter))\n",
    "    cal_acc(net)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer product training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar100_net.pth'\n",
    "net = VGG('VGG13').to(device)\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_ratio(net):\n",
    "    for l in [3, 6, 10, 13, 17, 20, 24, 27, 31, 34]:\n",
    "        t_mass, d_mass = cal_mass(net, l)\n",
    "        mass_ratio = d_mass/t_mass\n",
    "        print(\"Diagonal mass ratio for layer \" + str(l) + \": \" + str(mass_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mass(net, l_index):\n",
    "    \n",
    "    other_loss = 0.0\n",
    "    self_loss = 0.0\n",
    "    num_iter = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            num_iter += 1\n",
    "            if(num_iter == 40):\n",
    "                break\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            L_self = 0.0\n",
    "            L_mat = 0.0\n",
    "\n",
    "            for epoch_num in range(1):            \n",
    "                out_features = net.features[0:l_index](inputs)\n",
    "                act_mat = (out_features).reshape(out_features.shape[0], out_features.shape[1], -1)\n",
    "\n",
    "                mul_mat = (torch.matmul(act_mat,act_mat.permute(0,2,1)))\n",
    "\n",
    "                L_self += (mul_mat.diagonal(dim1=1,dim2=2).norm(dim=1).pow(2)).sum()\n",
    "                L_mat += (mul_mat.norm(dim=(1,2)).pow(2)).sum()\n",
    "\n",
    "            other_loss += L_mat\n",
    "            self_loss += L_self\n",
    "\n",
    "            del L_self, L_mat, act_mat, out_features\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return other_loss, self_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 1e-3\n",
    "# reg = ((mat_loss/num_iter).log()/np.log(10)).floor()\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    mat_loss = 0.0\n",
    "    num_iter = 0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        num_iter += 1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        L_mat = 0.0\n",
    "        \n",
    "#         for epoch_num in range(1):            \n",
    "        for l_index in [3, 6, 10, 13, 17, 20, 24, 27, 31, 34]: \n",
    "            out_features = net.features[0:l_index](inputs)\n",
    "            act_mat = (out_features).reshape(out_features.shape[0], out_features.shape[1], -1)\n",
    "\n",
    "            mul_mat = (torch.matmul(act_mat,act_mat.permute(0,2,1)))\n",
    "\n",
    "            L_mat += (mul_mat.norm(dim=(1,2)).pow(2) - mul_mat.diagonal(dim1=1,dim2=2).norm(dim=1).pow(2)).sum()\n",
    "#             L_mat += (mul_mat.pow(2).sum(dim=(1,2)) - mul_mat.pow(2).diagonal(dim1=1,dim2=2).sum(dim=1)).sum()\n",
    "\n",
    "        Lc = criterion(outputs, labels)\n",
    "        loss = Lc + reg*(L_mat)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        mat_loss += L_mat\n",
    "\n",
    "        del L_mat, act_mat, out_features\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    print(\"Regularization loss: \" + str(mat_loss/num_iter))\n",
    "    \n",
    "    m_ratio(net)\n",
    "    \n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "          (epoch + 1, i + 1, running_loss / num_iter))\n",
    "    cal_acc(net)\n",
    "    \n",
    "#     reg = 1e-9 #10**(-((mat_loss.detach().cpu()/num_iter).log()/np.log(10)).floor())\n",
    "    print(\"reg: \", reg)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import os\n",
    "# # os.mkdir(\"decorr\")\n",
    "PATH = './decorr/cifar100_decorrnet_8.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
