{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(45),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "     ])\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "     ])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./../data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./../data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg, classes=100):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, cfg[0], kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[0]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(cfg[0], cfg[1], kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[1]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(cfg[1], cfg[2], kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[2]),\n",
    "            nn.Conv2d(cfg[2], cfg[3], kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[3]),\n",
    "            nn.Conv2d(cfg[3], cfg[4], kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(cfg[4]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(cfg[4] * 1 * 1, cfg[5]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(cfg[5], cfg[6]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(cfg[6], classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_diag(net):\n",
    "    ### Conv_ind == 0 ###\n",
    "    w_mat = net.features[0].weight\n",
    "    w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "    b_mat = net.features[0].bias\n",
    "    b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "    params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "    angle_mat = torch.matmul(torch.t(params), params)\n",
    "    L_diag = (angle_mat.diag().norm(1))\n",
    "    L_angle = (angle_mat.norm(1))\n",
    "    print(L_diag.cpu()/L_angle.cpu())\n",
    "    \n",
    "    for conv_ind in [6, 10, 13, 16]:\n",
    "        w_mat = net.features[conv_ind-2].weight\n",
    "        w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "        b_mat = net.features[conv_ind-2].bias\n",
    "        b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "        params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "        angle_mat = torch.matmul(params, torch.t(params)) \n",
    "        L_diag = (angle_mat.diag().norm(1))\n",
    "        L_angle = (angle_mat.norm(1))\n",
    "        print(L_diag.cpu()/L_angle.cpu())\n",
    "\n",
    "    ### lin_ind = 1 ###        \n",
    "    w_mat = net.classifier[1].weight\n",
    "    w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "    b_mat = net.classifier[1].bias\n",
    "    b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))            \n",
    "    params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "    angle_mat = torch.matmul(torch.t(params), params)\n",
    "    L_diag = (angle_mat.diag().norm(1))\n",
    "    L_angle = (angle_mat.norm(1))\n",
    "    print(L_diag.cpu()/L_angle.cpu())\n",
    "\n",
    "    ### lin_ind = 4 ###        \n",
    "    w_mat = net.classifier[4].weight\n",
    "    w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "    b_mat = net.classifier[4].bias\n",
    "    b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))            \n",
    "    params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "    angle_mat = torch.matmul(params, torch.t(params))\n",
    "    L_diag = (angle_mat.diag().norm(1))\n",
    "    L_angle = (angle_mat.norm(1))\n",
    "    print(L_diag.cpu()/L_angle.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = [64, 192, 384, 256, 256, 4096, 4096]\n",
    "best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AlexNet(cfg).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def net_train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "def net_test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'best_acc': acc\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "\n",
    "__all__ = ['accuracy']\n",
    "\n",
    "def kaccuracy(output, target, topk=(5,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def top5cal(net):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    top1 = 0\n",
    "    top5 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            acc1, acc5 = kaccuracy(outputs, targets, topk=(1, 5))\n",
    "            top1 += (acc1.item()*inputs.shape[0])\n",
    "            top5 += (acc5.item()*inputs.shape[0])\n",
    "    top1 /= 10000\n",
    "    top5 /= 10000\n",
    "    \n",
    "    print(\"top5\", top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_dict = torch.load('./checkpoint/ckpt.pth')\n",
    "net.load_state_dict(net_dict['net'])\n",
    "best_acc = net_dict['best_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.000001, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      " [========================>]  Step: 25ms | Tot: 11s232ms | Loss: 1.360 | Acc: 61.852% (30926/5000 391/391 ........]  Step: 28ms | Tot: 270ms | Loss: 1.352 | Acc: 60.547% (775/128 10/391 ..................]  Step: 30ms | Tot: 358ms | Loss: 1.380 | Acc: 59.856% (996/166 13/391 ..................]  Step: 28ms | Tot: 387ms | Loss: 1.372 | Acc: 60.156% (1078/179 14/391 ...................]  Step: 29ms | Tot: 416ms | Loss: 1.377 | Acc: 60.104% (1154/192 15/391 ......................]  Step: 29ms | Tot: 529ms | Loss: 1.369 | Acc: 60.485% (1471/243 19/391 ...................]  Step: 28ms | Tot: 557ms | Loss: 1.366 | Acc: 60.742% (1555/256 20/391 22/391 .....................]  Step: 29ms | Tot: 644ms | Loss: 1.369 | Acc: 60.734% (1788/294 23/391 ]  Step: 29ms | Tot: 757ms | Loss: 1.373 | Acc: 60.388% (2087/345 27/391 .............]  Step: 31ms | Tot: 814ms | Loss: 1.374 | Acc: 60.264% (2237/371 29/39 31/391 .......................]  Step: 27ms | Tot: 899ms | Loss: 1.385 | Acc: 60.156% (2464/409 32/391 ==>.....................]  Step: 30ms | Tot: 1s451ms | Loss: 1.393 | Acc: 60.616% (3957/652 51/391 >.....................]  Step: 29ms | Tot: 1s537ms | Loss: 1.397 | Acc: 60.590% (4188/691 54/391 >....................]  Step: 28ms | Tot: 2s240ms | Loss: 1.383 | Acc: 60.917% (6082/998 78/391 ..............]  Step: 28ms | Tot: 2s268ms | Loss: 1.381 | Acc: 60.918% (6160/1011 79/391 =========>..........]  Step: 31ms | Tot: 6s378ms | Loss: 1.373 | Acc: 61.494% (17474/2841 222/391 =======>..........]  Step: 27ms | Tot: 6s578ms | Loss: 1.376 | Acc: 61.436% (18008/29 229/391 231/39 233/391 ===============>........]  Step: 32ms | Tot: 7s410ms | Loss: 1.370 | Acc: 61.573% (20334/3302 258/391 ==================>....]  Step: 29ms | Tot: 9s141ms | Loss: 1.366 | Acc: 61.736% (25129/4070 318/391 =====================>.]  Step: 31ms | Tot: 10s514ms | Loss: 1.361 | Acc: 61.887% (28993/4684 366/391 ===============>.]  Step: 29ms | Tot: 10s799ms | Loss: 1.360 | Acc: 61.883% (29783/4812 376/391 \n",
      " [========================>]  Step: 9ms | Tot: 1s683ms | Loss: 1.951 | Acc: 50.660% (5066/1000 79/79 ================>........]  Step: 15ms | Tot: 1s139ms | Loss: 1.942 | Acc: 50.722% (3441/678 53/79 \n",
      "\n",
      "Epoch: 2\n",
      " [========================>]  Step: 28ms | Tot: 11s249ms | Loss: 1.363 | Acc: 61.704% (30852/5000 391/391 91 .....]  Step: 26ms | Tot: 3s131ms | Loss: 1.387 | Acc: 60.732% (8551/1408 110/391 112/391 >.................]  Step: 30ms | Tot: 3s218ms | Loss: 1.387 | Acc: 60.723% (8783/1446 113/391 ........]  Step: 29ms | Tot: 3s247ms | Loss: 1.387 | Acc: 60.739% (8863/1459 114/391 .................]  Step: 29ms | Tot: 3s306ms | Loss: 1.384 | Acc: 60.769% (9023/1484 116/391 ============>...........]  Step: 30ms | Tot: 5s898ms | Loss: 1.381 | Acc: 61.113% (16036/2624 205/391   Step: 30ms | Tot: 6s483ms | Loss: 1.375 | Acc: 61.215% (17630/2880 225/391 ============>..........]  Step: 26ms | Tot: 6s772ms | Loss: 1.373 | Acc: 61.247% (18423/3008 235/391 .....]  Step: 28ms | Tot: 6s831ms | Loss: 1.374 | Acc: 61.234% (18576/3033 237/39 239/391 ===========>.........]  Step: 28ms | Tot: 6s916ms | Loss: 1.373 | Acc: 61.270% (18822/3072 240/391 ============>.........]  Step: 29ms | Tot: 6s945ms | Loss: 1.373 | Acc: 61.275% (18902/3084 241/39 242/391 ============>.........]  Step: 29ms | Tot: 7s30ms | Loss: 1.373 | Acc: 61.312% (19149/3123 244/3 247/3 249/391 =============>.........]  Step: 31ms | Tot: 7s202ms | Loss: 1.372 | Acc: 61.322% (19623/3200 250/391 295/391 ]  Step: 25ms | Tot: 10s387ms | Loss: 1.364 | Acc: 61.730% (28524/4620 361/391 ==================>.]  Step: 27ms | Tot: 10s445ms | Loss: 1.364 | Acc: 61.725% (28680/4646 363/391 =======================>]  Step: 29ms | Tot: 10s852ms | Loss: 1.363 | Acc: 61.746% (29796/4825 377/391 ======================>]  Step: 32ms | Tot: 11s82ms | Loss: 1.364 | Acc: 61.698% (30405/4928 385/391 \n",
      " [========================>]  Step: 11ms | Tot: 1s653ms | Loss: 1.946 | Acc: 50.740% (5074/1000 79/79 9 .]  Step: 28ms | Tot: 239ms | Loss: 2.007 | Acc: 49.459% (823/166 13/79 .....]  Step: 11ms | Tot: 251ms | Loss: 2.000 | Acc: 49.777% (892/179 14/79 ...]  Step: 28ms | Tot: 279ms | Loss: 1.981 | Acc: 49.948% (959/192 15/79 ===>....................]  Step: 12ms | Tot: 292ms | Loss: 1.954 | Acc: 50.439% (1033/204 16/79 ...........]  Step: 27ms | Tot: 319ms | Loss: 1.956 | Acc: 50.276% (1094/217 17/79 ..............]  Step: 28ms | Tot: 359ms | Loss: 1.969 | Acc: 50.041% (1217/243 19/79 =>..................]  Step: 27ms | Tot: 399ms | Loss: 1.951 | Acc: 50.149% (1348/268 21/79 69/79 \n",
      "\n",
      "Epoch: 3\n",
      " [========================>]  Step: 26ms | Tot: 11s294ms | Loss: 1.368 | Acc: 61.788% (30894/5000 391/391 ==>..................]  Step: 25ms | Tot: 2s765ms | Loss: 1.383 | Acc: 61.364% (7619/1241 97/391 =======>................]  Step: 34ms | Tot: 3s957ms | Loss: 1.377 | Acc: 61.628% (10886/1766 138/391 ==========>.............]  Step: 33ms | Tot: 5s347ms | Loss: 1.378 | Acc: 61.610% (14668/2380 186/391 =============>..........]  Step: 26ms | Tot: 6s355ms | Loss: 1.378 | Acc: 61.496% (17396/2828 221/391 ======================>]  Step: 30ms | Tot: 11s90ms | Loss: 1.367 | Acc: 61.823% (30387/4915 384/391 \n",
      " [========================>]  Step: 10ms | Tot: 1s694ms | Loss: 1.949 | Acc: 50.840% (5084/1000 79/79 \n",
      "\n",
      "Epoch: 4\n",
      " [========================>]  Step: 25ms | Tot: 11s249ms | Loss: 1.367 | Acc: 61.800% (30900/5000 391/391 ...................]  Step: 27ms | Tot: 1s278ms | Loss: 1.415 | Acc: 60.208% (3468/576 45/39 103/391 ================>.......]  Step: 29ms | Tot: 7s814ms | Loss: 1.374 | Acc: 61.543% (21348/3468 271/391 \n",
      " [========================>]  Step: 9ms | Tot: 1s643ms | Loss: 1.949 | Acc: 50.840% (5084/1000 79/79 /7 56/79 \n",
      "\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [========================>]  Step: 28ms | Tot: 11s233ms | Loss: 1.362 | Acc: 61.684% (30842/5000 391/391 ===>.....................]  Step: 29ms | Tot: 1s617ms | Loss: 1.408 | Acc: 60.540% (4417/729 57/391 ...........]  Step: 27ms | Tot: 1s645ms | Loss: 1.407 | Acc: 60.601% (4499/742 58/391 ..................]  Step: 29ms | Tot: 1s674ms | Loss: 1.407 | Acc: 60.620% (4578/755 59/391 =>.....................]  Step: 31ms | Tot: 1s733ms | Loss: 1.409 | Acc: 60.630% (4734/780 61/391 ..................]  Step: 28ms | Tot: 1s817ms | Loss: 1.405 | Acc: 60.669% (4970/819 64/391 >....................]  Step: 27ms | Tot: 1s845ms | Loss: 1.404 | Acc: 60.673% (5048/832 65/391 ==>....................]  Step: 28ms | Tot: 1s962ms | Loss: 1.399 | Acc: 60.892% (5378/883 69/391 ==>....................]  Step: 28ms | Tot: 2s19ms | Loss: 1.396 | Acc: 60.904% (5535/908 71/391 ...]  Step: 29ms | Tot: 2s77ms | Loss: 1.395 | Acc: 60.970% (5697/934 73/391 .............]  Step: 28ms | Tot: 2s219ms | Loss: 1.397 | Acc: 60.757% (6066/998 78/39 80/391 ...]  Step: 30ms | Tot: 2s392ms | Loss: 1.390 | Acc: 60.882% (6546/1075 84/391 ..................]  Step: 27ms | Tot: 2s420ms | Loss: 1.392 | Acc: 60.763% (6611/1088 85/391 ===>...................]  Step: 28ms | Tot: 2s449ms | Loss: 1.391 | Acc: 60.774% (6690/1100 86/391 ..............]  Step: 28ms | Tot: 2s477ms | Loss: 1.387 | Acc: 60.848% (6776/1113 87/391 =>..................]  Step: 29ms | Tot: 2s893ms | Loss: 1.387 | Acc: 60.899% (7873/1292 101/39 107/391 108/391 118/391 124/391 ==>................]  Step: 30ms | Tot: 3s759ms | Loss: 1.386 | Acc: 60.914% (10214/1676 131/391 ............]  Step: 30ms | Tot: 3s928ms | Loss: 1.384 | Acc: 60.920% (10683/1753 137/391 141/391 =====>...............]  Step: 26ms | Tot: 4s69ms | Loss: 1.388 | Acc: 60.794% (11050/1817 142/391 >..............]  Step: 28ms | Tot: 4s644ms | Loss: 1.384 | Acc: 60.991% (12647/2073 162/391 =====>..............]  Step: 28ms | Tot: 4s759ms | Loss: 1.382 | Acc: 61.069% (12976/2124 166/39 168/391 ...]  Step: 30ms | Tot: 4s957ms | Loss: 1.383 | Acc: 61.037% (13516/2214 173/39 183/391 187/391 191/39 201/39 203/39 205/391 209/391 225/391 ===================>...]  Step: 29ms | Tot: 9s580ms | Loss: 1.367 | Acc: 61.625% (26346/4275 334/391 ====================>...]  Step: 29ms | Tot: 9s638ms | Loss: 1.368 | Acc: 61.591% (26489/4300 336/391 ====================>.]  Step: 33ms | Tot: 10s464ms | Loss: 1.364 | Acc: 61.667% (28732/4659 364/391 =====================>.]  Step: 28ms | Tot: 10s519ms | Loss: 1.363 | Acc: 61.710% (28910/4684 366/391 368/391 \n",
      " [========================>]  Step: 11ms | Tot: 1s667ms | Loss: 1.952 | Acc: 50.870% (5087/100077/79 3/79 70/7 78/7 79/79 \n"
     ]
    }
   ],
   "source": [
    "for whatev in range(1):\n",
    "    for epoch in range(5):\n",
    "        net_train(epoch+1)\n",
    "        net_test(epoch+1)\n",
    "\n",
    "#     net_dict = torch.load('./checkpoint/ckpt.pth')\n",
    "#     net.load_state_dict(net_dict['net'])\n",
    "#     best_acc = net_dict['best_acc']\n",
    "\n",
    "#     optimizer = optim.Adam(net.parameters(), lr=0.000001, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inner product training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_ortho = AlexNet(cfg).to(device)\n",
    "net_dict = torch.load('./ortho_checkpoint/ckpt.pth')\n",
    "net_ortho.load_state_dict(net_dict['net'])\n",
    "best_acc_ortho = net_dict['best_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_imp = {}\n",
    "\n",
    "for conv_ind in [2, 6, 10, 13, 16]:\n",
    "    l_imp.update({conv_ind: net.features[conv_ind].bias.shape[0]})\n",
    "    \n",
    "for lin_ind in [1, 4]:\n",
    "    l_imp.update({lin_ind: net.classifier[lin_ind].bias.shape[0]})\n",
    "    \n",
    "normalizer = 0\n",
    "for key, val in l_imp.items():\n",
    "    normalizer += val\n",
    "for key, val in l_imp.items():\n",
    "    l_imp[key] = val / normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_test_ortho(epoch):\n",
    "    global best_acc_ortho\n",
    "    net_ortho.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net_ortho(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    print(acc)\n",
    "    if acc > best_acc_ortho:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net_ortho.state_dict(),\n",
    "            'best_acc': acc\n",
    "        }\n",
    "        if not os.path.isdir('ortho_checkpoint'):\n",
    "            os.mkdir('ortho_checkpoint')\n",
    "        torch.save(state, './ortho_checkpoint/ckpt.pth')\n",
    "        best_acc_ortho = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_train_ortho(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net_ortho.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    angle_cost = 0.0\n",
    "            \n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net_ortho(inputs)\n",
    "        L_angle = 0\n",
    "        \n",
    "        ### Conv_ind == 0 ###\n",
    "        w_mat = net_ortho.features[0].weight\n",
    "        w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "        b_mat = net_ortho.features[0].bias\n",
    "        b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "        params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "        angle_mat = torch.matmul(torch.t(params), params) - torch.eye(params.shape[1]).to(device)\n",
    "        L_angle += (l_imp[2])*(angle_mat).norm(1) #.norm().pow(2))\n",
    "\n",
    "        ### Conv_ind != 0 ###\n",
    "        for conv_ind in [6, 10, 13, 16]:\n",
    "            w_mat = net_ortho.features[conv_ind-2].weight\n",
    "            w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "            b_mat = net_ortho.features[conv_ind-2].bias\n",
    "            b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "            params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "            angle_mat = torch.matmul(params, torch.t(params)) - torch.eye(w_mat.shape[0]).to(device)            \n",
    "            L_angle += (l_imp[conv_ind])*(angle_mat).norm(1) #.norm().pow(2))\n",
    "    \n",
    "        ### lin_ind = 1 ###        \n",
    "        w_mat = net_ortho.classifier[1].weight\n",
    "        w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "        b_mat = net_ortho.classifier[1].bias\n",
    "        b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))            \n",
    "        params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "        angle_mat = torch.matmul(torch.t(params), params) - torch.eye(params.shape[1]).to(device)\n",
    "        L_angle += (l_imp[1])*(angle_mat).norm(1) #.norm().pow(2))\n",
    "        \n",
    "        ### lin_ind = 4 ###        \n",
    "        w_mat = net_ortho.classifier[4].weight\n",
    "        w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "        b_mat = net_ortho.classifier[4].bias\n",
    "        b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))            \n",
    "        params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "        angle_mat = torch.matmul(params, torch.t(params)) - torch.eye(params.shape[0]).to(device)\n",
    "        L_angle += (l_imp[4])*(angle_mat).norm(1) #.norm().pow(2))        \n",
    "        \n",
    "        Lc = criterion(outputs, labels)\n",
    "        loss = (1e-1)*(L_angle) + Lc\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        angle_cost += (L_angle).item()\n",
    "    \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (running_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    \n",
    "    print(\"angle_cost: \", angle_cost/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net_ortho.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " [========================>]  Step: 122ms | Tot: 46s865ms | Loss: 316.814 | Acc: 53.922% (26961/5000 391/391 \n",
      "angle_cost:  24.584421733398436\n",
      " [========================>]  Step: 10ms | Tot: 1s793ms | Loss: 3.030 | Acc: 47.580% (4758/1000 79/79 \n",
      "47.58\n",
      "Saving..\n",
      "tensor(0.2245, grad_fn=<DivBackward0>)\n",
      "tensor(0.1624, grad_fn=<DivBackward0>)\n",
      "tensor(0.7100, grad_fn=<DivBackward0>)\n",
      "tensor(0.8802, grad_fn=<DivBackward0>)\n",
      "tensor(0.7909, grad_fn=<DivBackward0>)\n",
      "tensor(0.8893, grad_fn=<DivBackward0>)\n",
      "tensor(0.5713, grad_fn=<DivBackward0>)\n",
      "\n",
      "Epoch: 1\n",
      " [====================>....]  Step: 123ms | Tot: 39s772ms | Loss: 186.963 | Acc: 45.953% (19293/4198 328/391 \r"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    net_train_ortho(epoch)\n",
    "    net_test_ortho(epoch)\n",
    "    w_diag(net_ortho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './w_decorr/base_params/wnet_base_2.pth'\n",
    "# torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_index = 10\n",
    "layer_id = 'bn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlated Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = AlexNet(cfg).to(device)\n",
    "net_dict = torch.load('./checkpoint/ckpt.pth')\n",
    "net.load_state_dict(net_dict['net'])\n",
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_base = net.features[l_index].weight.data.clone().detach()\n",
    "bias_base = net.features[l_index].bias.data.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_base_corr = 0\n",
    "num_stop = 0\n",
    "for epoch in range(1):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_base_corr += loss.item()\n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break\n",
    "loss_base_corr = loss_base_corr**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mat_corr = torch.zeros(weight_base.shape[0])\n",
    "\n",
    "for n_index in range(weight_base.shape[0]): \n",
    "    num_stop = 0\n",
    "    print(n_index)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    net.features[l_index].weight.data[n_index] = 0 #torch.zeros((weight_base.shape[1],weight_base.shape[2],weight_base.shape[3]))\n",
    "    net.features[l_index].bias.data[n_index] = 0 #torch.zeros((weight_base.shape[1],weight_base.shape[2],weight_base.shape[3]))\n",
    "    \n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = (criterion(outputs, labels))\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break\n",
    "            \n",
    "    loss_mat_corr[n_index] = running_loss**2\n",
    "    \n",
    "    net.features[l_index].weight.data = weight_base.clone().detach()\n",
    "    net.features[l_index].bias.data = bias_base.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(loss_mat_corr, './w_decorr/loss_mats/corr/'+str(l_index)+'/loss_corr_bn_train_'+str(l_index)+'.pt')\n",
    "loss_mat_corr = torch.load('./w_decorr/loss_mats/corr/'+str(l_index)+'/loss_corr_bn_train_'+str(l_index)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0, weight_decay=0)\n",
    "av_corrval = 0\n",
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    num_stop = 0\n",
    "    running_loss = 0.0\n",
    "    imp_corr_conv = torch.zeros(bias_base.shape[0]).to(device)\n",
    "    imp_corr_bn = torch.zeros(bias_base.shape[0]).to(device)\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        imp_corr_bn += (((net.features[l_index].weight.grad)*(net.features[l_index].weight.data)) + ((net.features[l_index].bias.grad)*(net.features[l_index].bias.data))).abs().pow(2)\n",
    "        \n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break\n",
    "         \n",
    "    corrval = (np.corrcoef(imp_corr_bn.cpu().detach().numpy(), (loss_mat_corr - loss_base_corr).abs().cpu().detach().numpy()))\n",
    "    print(\"Correlation at epoch \"+str(epoch)+\": \"+str(corrval[0,1]))\n",
    "    av_corrval += corrval[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorrelated net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_decorr = AlexNet(cfg).to(device)\n",
    "net_dict = torch.load('./ortho_checkpoint/ckpt.pth')\n",
    "net_decorr.load_state_dict(net_dict['net'])\n",
    "net_decorr = net_decorr.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_base = net_decorr.features[l_index].weight.data.clone().detach()\n",
    "bias_base = net_decorr.features[l_index].bias.data.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "num_stop = 0\n",
    "loss_base_decorr = 0\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net_decorr(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_base_decorr += loss.item()\n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break\n",
    "loss_base_decorr = loss_base_decorr**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "\n",
    "loss_mat_decorr = torch.zeros(weight_base.shape[0])\n",
    "\n",
    "for n_index in range(weight_base.shape[0]): \n",
    "    print(n_index)\n",
    "    num_stop = 0\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        net_decorr.features[l_index].weight.data[n_index] = 0 #torch.zeros((weight_base.shape[1],weight_base.shape[2],weight_base.shape[3]))\n",
    "        net_decorr.features[l_index].bias.data[n_index] = 0 #torch.zeros((weight_base.shape[1],weight_base.shape[2],weight_base.shape[3]))\n",
    "        outputs = net_decorr(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break\n",
    "            \n",
    "    loss_mat_decorr[n_index] = running_loss**2\n",
    "    \n",
    "    net_decorr.features[l_index].weight.data = weight_base.clone().detach()\n",
    "    net_decorr.features[l_index].bias.data = bias_base.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(loss_mat_decorr, './w_decorr/loss_mats/decorr/'+str(l_index)+'/loss_decorr_bn_train_'+str(l_index)+'.pt')\n",
    "loss_mat_decorr = torch.load('./w_decorr/loss_mats/decorr/'+str(l_index)+'/loss_decorr_bn_train_'+str(l_index)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "av_corrval = 0\n",
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    num_stop = 0\n",
    "    imp_decorr_conv = torch.zeros(bias_base.shape[0]).to(device)\n",
    "    imp_decorr_bn = torch.zeros(bias_base.shape[0]).to(device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net_decorr(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break\n",
    "        \n",
    "        imp_decorr_bn += (((net_decorr.features[l_index].weight.grad)*(net_decorr.features[l_index].weight.data)) + ((net_decorr.features[l_index].bias.grad)*(net_decorr.features[l_index].bias.data))).abs().pow(2)\n",
    "    \n",
    "    corrval = (np.corrcoef(imp_decorr_bn.cpu().detach().numpy(), (loss_mat_decorr - loss_base_decorr).abs().cpu().detach().numpy()))\n",
    "    print(\"Correlation at epoch \"+str(epoch)+\": \"+str(corrval[0,1]))\n",
    "    av_corrval += corrval[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "s = imp_corr_bn.cpu().sort()[0].cpu().numpy()\n",
    "order = imp_corr_bn.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max(), label=\"Estimated importance\")\n",
    "plt.title(\"Correlated (Taylor FO) for \"+str(l_index))\n",
    "loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "plt.xlabel(\"Neuron index\")\n",
    "plt.ylabel(\"Normalized importance\")\n",
    "plt.plot(loss_diff[order]/loss_diff.max(), label=\"Actual importance\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./w_decorr/loss_mats/corr/graphs/\"+str(l_index)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "s = imp_decorr_bn.cpu().sort()[0].cpu().numpy()\n",
    "order = imp_decorr_bn.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max(), label=\"Estimated importance\")\n",
    "plt.title(\"Decorrelated (Taylor FO) for \"+str(l_index))\n",
    "loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "plt.xlabel(\"Neuron index\")\n",
    "plt.ylabel(\"Normalized importance\")\n",
    "plt.plot(loss_diff[order]/loss_diff.max(), label=\"Actual importance\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./w_decorr/loss_mats/decorr/graphs/\"+str(l_index)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = imp_decorr_bn.cpu().sort()[0].cpu().numpy()\n",
    "s = s/s.max()\n",
    "order = imp_decorr_bn.sort()[1].cpu().numpy()\n",
    "loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "loss_diff = (loss_diff[order]/loss_diff.max())\n",
    "ortho_rms = ((loss_diff - s)**2).sum()\n",
    "\n",
    "s = imp_corr_bn.cpu().sort()[0].cpu().numpy()\n",
    "s = s/s.max()\n",
    "order = imp_corr_bn.sort()[1].cpu().numpy()\n",
    "loss_diff = (loss_mat_corr - loss_base_corr).abs()\n",
    "loss_diff = (loss_diff[order]/loss_diff.max())\n",
    "\n",
    "base_rms = ((loss_diff - s)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ortho_rms, base_rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rms_ortho = np.sqrt(np.array(rms_ortho) / np.array([64, 64, 128, 128, 256, 256, 512, 512, 512, 512]))\n",
    "# rms_base = np.sqrt(np.array(rms_base) / np.array([64, 64, 128, 128, 256, 256, 512, 512, 512, 512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(np.linspace(0,30,10)-0.5, rms_ortho, label=\"Decorrelated network\")\n",
    "plt.bar(np.linspace(0,30,10)+0.5, rms_base, label=\"Correlated network\")\n",
    "plt.xlabel(\"Layer ID\")\n",
    "plt.ylabel(\"RMS\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./w_decorr/loss_mats/rms.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20,5))\n",
    "\n",
    "s = imp_decorr_bn.cpu().sort()[0].cpu().numpy()\n",
    "order = imp_decorr_bn.sort()[1].cpu().numpy()\n",
    "axes[0].plot(s/s.max(), label=\"Estimated importance\")\n",
    "axes[0].set_title(\"Decorrelated Network (layer \"+str(l_index)+\")\")\n",
    "loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "axes[0].set_xlabel(\"Neuron index\")\n",
    "axes[0].set_ylabel(\"Normalized importance\")\n",
    "axes[0].plot(loss_diff[order]/loss_diff.max(), label=\"Actual importance\")\n",
    "axes[0].legend()\n",
    "\n",
    "s = imp_corr_bn.cpu().sort()[0].cpu().numpy()\n",
    "order = imp_corr_bn.sort()[1].cpu().numpy()\n",
    "axes[1].plot(s/s.max(), label=\"Estimated importance\")\n",
    "axes[1].set_title(\"Correlated Network (layer \"+str(l_index)+\")\")\n",
    "loss_diff = (loss_mat_corr - loss_base_corr).abs()\n",
    "axes[1].set_xlabel(\"Neuron index\")\n",
    "axes[1].set_ylabel(\"Normalized importance\")\n",
    "axes[1].plot(loss_diff[order]/loss_diff.max(), label=\"Actual importance\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.savefig(\"./w_decorr/loss_mats/subplots/\"+str(l_index)+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net-Slim Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_corr = net.features[l_index].weight.data.clone()\n",
    "np.corrcoef(scale_corr.cpu().numpy(), (loss_mat_corr - loss_base_corr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_decorr = net_decorr.features[l_index].weight.data.clone().abs()\n",
    "np.corrcoef((scale_decorr).cpu().numpy(), (loss_mat_decorr - loss_base_decorr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 based pruning Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_corr = net.features[l_index - 2].weight.data.clone()\n",
    "w_imp_corr = w_corr.pow(2).sum(dim=(1,2,3)).cpu()\n",
    "np.corrcoef(w_imp_corr.numpy(), (loss_mat_corr - loss_base_corr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_decorr = net_decorr.features[l_index - 2].weight.data.clone()\n",
    "w_imp_decorr = w_decorr.pow(2).sum(dim=(1,2,3)).cpu()\n",
    "w_imp_decorr = (w_imp_decorr - w_imp_decorr.min())\n",
    "w_imp_decorr = w_imp_decorr/w_imp_decorr.max()\n",
    "np.corrcoef(w_imp_decorr.numpy(), (loss_mat_decorr - loss_base_decorr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance plots Netslim Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "\n",
    "s = scale_corr.cpu().sort()[0].cpu().numpy()\n",
    "order = scale_corr.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Correlated (Net-Slim)\")\n",
    "loss_diff = (loss_mat_corr - loss_base_corr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "\n",
    "s = scale_decorr.cpu().sort()[0].cpu().numpy()\n",
    "order = scale_decorr.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Decorrelated (Net-Slim)\")\n",
    "loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance plots L2 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "s = w_imp_corr.sort()[0].cpu().numpy()\n",
    "order = w_imp_corr.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Correlated (L2)\")\n",
    "loss_diff = (loss_mat_corr - loss_base_corr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "s = w_imp_decorr.sort()[0].cpu().numpy()\n",
    "order = w_imp_decorr.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Decorrelated (L2)\")\n",
    "loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
