{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from models import *\n",
    "from utils import progress_bar\n",
    "from imp_baselines import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./../data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./../data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and load base networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "print('==> Building model..')\n",
    "net_corr = MobileNet()\n",
    "net_decorr = MobileNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "net_corr = net_corr.to(device)\n",
    "net_decorr = net_decorr.to(device)\n",
    "if device == 'cuda':\n",
    "    net_corr = torch.nn.DataParallel(net_corr)\n",
    "    net_decorr = torch.nn.DataParallel(net_decorr)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_corr = './w_decorr/base_params/cifar100_net.pth'\n",
    "net_dict = torch.load(PATH_corr)\n",
    "net_corr.load_state_dict(net_dict['net'])\n",
    "\n",
    "PATH_decorr = './w_decorr/base_params/wnet_base.pth'\n",
    "net_dict = torch.load(PATH_decorr)\n",
    "net_decorr.load_state_dict(net_dict['net_ortho'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(net_test):\n",
    "    net_test.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net_test(inputs)\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print(100 * correct / total)\n",
    "        \n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_importance(net, l_id):\n",
    "    bias_base = l_id.bias.data.clone().detach()\n",
    "    av_corrval = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    imp_corr_bn = torch.zeros(bias_base.shape[0]).to(device)\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        imp_corr_bn += (((l_id.weight.grad)*(l_id.weight.data)) + ((l_id.bias.grad)*(l_id.bias.data))).abs().pow(2)\n",
    "        break\n",
    "    imp_norm = imp_corr_bn\n",
    "    \n",
    "    neuron_order = [np.linspace(0, imp_norm.shape[0]-1, imp_norm.shape[0]), imp_norm]\n",
    "    \n",
    "    return neuron_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_time(net_acc):\n",
    "    net_acc.eval()\n",
    "    testsamp = torch.rand(1,3,32,32).to(device)\n",
    "    \n",
    "    for i in range(5):\n",
    "        net_acc(testsamp)    \n",
    "    t_end = 0\n",
    "    t_s = time.time()\n",
    "    for i in range(25):\n",
    "        net_acc(testsamp)\n",
    "        t_end += time.time() - t_s\n",
    "    \n",
    "    return (t_end / 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_corr = cal_time(net_corr)\n",
    "t_decorr = cal_time(net_decorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFO importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./w_decorr/base_params/tfo_corr.pkl\", 'rb') as f:\n",
    "    imp_order_corr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_corr.parameters(), lr=0, weight_decay=0)\n",
    "imp_order_corr = np.array([[],[],[]]).transpose()\n",
    "i = 0\n",
    "\n",
    "print(i)\n",
    "nlist = cal_importance(net_corr, net_corr.module.bn1)\n",
    "imp_order_corr = np.concatenate((imp_order_corr,np.array([np.repeat([i],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "i+=1\n",
    "\n",
    "for l_index in range(13):\n",
    "    print(i)\n",
    "    nlist = cal_importance(net_corr, net_corr.module.layers[l_index].bn1)\n",
    "    imp_order_corr = np.concatenate((imp_order_corr,np.array([np.repeat([i],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "    print(i)\n",
    "    nlist = cal_importance(net_corr, net_corr.module.layers[l_index].bn2)\n",
    "    imp_order_corr = np.concatenate((imp_order_corr,np.array([np.repeat([i],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "with open(\"./w_decorr/base_params/tfo_corr.pkl\", 'wb') as f:\n",
    "    pickle.dump(imp_order_corr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./w_decorr/base_params/tfo_w_decorr.pkl\", 'rb') as f:\n",
    "    imp_order_decorr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "imp_order_decorr = np.array([[],[],[]]).transpose()\n",
    "i = 0\n",
    "\n",
    "print(i)\n",
    "nlist = cal_importance(net_decorr, net_decorr.module.bn1)\n",
    "imp_order_decorr = np.concatenate((imp_order_decorr,np.array([np.repeat([i],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "i+=1\n",
    "\n",
    "for l_index in range(13):\n",
    "    print(i)\n",
    "    nlist = cal_importance(net_decorr, net_decorr.module.layers[l_index].bn1)\n",
    "    imp_order_decorr = np.concatenate((imp_order_decorr,np.array([np.repeat([i],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "    print(i)\n",
    "    nlist = cal_importance(net_decorr, net_decorr.module.layers[l_index].bn2)\n",
    "    imp_order_decorr = np.concatenate((imp_order_decorr,np.array([np.repeat([i],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "with open(\"./w_decorr/base_params/tfo_w_decorr.pkl\", 'wb') as f:\n",
    "    pickle.dump(imp_order_decorr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order and ratios for pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_and_ratios(imp_order, prune_ratio):\n",
    "    imp_sort = np.argsort(imp_order[:,2])\n",
    "    temp_order = imp_order[imp_sort]\n",
    "\n",
    "    n_prune = int(prune_ratio * imp_order.shape[0])\n",
    "\n",
    "    prune_list = temp_order[0:n_prune]\n",
    "\n",
    "    imp_order_tfo = {}\n",
    "    ratios = []\n",
    "\n",
    "    for l_index in range(27):\n",
    "        nlist = temp_order[(temp_order[:,0] == l_index), 1].astype(int)\n",
    "        imp_order_tfo.update({l_index: nlist})\n",
    "        nlist = np.sort(prune_list[(prune_list[:,0] == l_index), 1].astype(int))\n",
    "        ratios.append(nlist.shape[0])\n",
    "    return imp_order_tfo, ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_size(net_size):\n",
    "    orig_size = [net_size.module.bn1.bias.shape[0]]\n",
    "\n",
    "    for l_index in range(13):\n",
    "        orig_size.append(net_size.module.layers[l_index].bn1.bias.shape[0])\n",
    "        orig_size.append(net_size.module.layers[l_index].bn2.bias.shape[0])\n",
    "\n",
    "    orig_size = np.array(orig_size)\n",
    "    \n",
    "    return orig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruned MobileNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfg_p(prune_ratio, orig_size, save_cfg=0):\n",
    "    cfg_list = []\n",
    "\n",
    "    for i in range(0,27,2):\n",
    "        cfg_list.append(int(orig_size[i] - prune_ratio[i]))\n",
    "    \n",
    "    for i in [2, 4, 6, 12]:\n",
    "        cfg_list[i] = (cfg_list[i], 2)\n",
    "    \n",
    "    if(save_cfg == 1):\n",
    "        with open(\"./w_decorr/pruned_nets/corr/cfgs/net_p_corr_iter\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(cfg_list, f)\n",
    "\n",
    "    elif(save_cfg == 2):\n",
    "        with open(\"./w_decorr/pruned_nets/decorr/cfgs/net_p_decorr_iter\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(cfg_list, f)\n",
    "    \n",
    "    return cfg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Depthwise conv + Pointwise conv'''\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileNet_p(nn.Module):\n",
    "\n",
    "    def __init__(self, in_first, cfg, num_classes=100):\n",
    "        super(MobileNet_p, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.conv1 = nn.Conv2d(3, in_first, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_first)\n",
    "        self.layers = self._make_layers(in_planes=in_first)\n",
    "        self.linear = nn.Linear(cfg[-1], num_classes)\n",
    "\n",
    "    def _make_layers(self, in_planes):\n",
    "        layers = []\n",
    "        for x in self.cfg:\n",
    "            out_planes = x if isinstance(x, int) else x[0]\n",
    "            stride = 1 if isinstance(x, int) else x[1]\n",
    "            layers.append(Block(in_planes, out_planes, stride))\n",
    "            in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruner(net, imp_order, prune_ratio, orig_size, net_type=0):\n",
    "    \n",
    "    cfg = cfg_p(prune_ratio, orig_size, save_cfg=net_type)\n",
    "    \n",
    "    net_pruned = torch.nn.DataParallel(MobileNet_p(cfg[0], cfg[1:]))\n",
    "    n_c = 0\n",
    "    order_c = []\n",
    "    order_p = []\n",
    "\n",
    "    for l in range(0, 27, 2):\n",
    "        if(l == 0):\n",
    "            n_c = prune_ratio[l]\n",
    "            order_c = np.sort(imp_order[l][n_c:])\n",
    "            net_pruned.module.conv1.weight.data = net.module.conv1.weight[order_c].data.detach().clone()\n",
    "            \n",
    "            net_pruned.module.bn1.weight.data = net.module.bn1.weight[order_c].data.detach().clone()\n",
    "            net_pruned.module.bn1.bias.data = net.module.bn1.bias[order_c].data.detach().clone()\n",
    "            net_pruned.module.bn1.running_var.data = net.module.bn1.running_var[order_c].data.detach().clone()\n",
    "            net_pruned.module.bn1.running_mean.data = net.module.bn1.running_mean[order_c].data.detach().clone()\n",
    "            \n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            ind = int((l+1)/2) - 1\n",
    "            # n_p = prune_ratio[ind]\n",
    "            # order_c = np.sort(imp_order[ind][n_p:])\n",
    "            net_pruned.module.layers[ind].conv1.weight.data = net.module.layers[ind].conv1.weight[order_c].data.detach().clone()\n",
    "            \n",
    "            net_pruned.module.layers[ind].bn1.weight.data = net.module.layers[ind].bn1.weight[order_c].data.detach().clone()\n",
    "            net_pruned.module.layers[ind].bn1.bias.data = net.module.layers[ind].bn1.bias[order_c].data.detach().clone()\n",
    "            net_pruned.module.layers[ind].bn1.running_var.data = net.module.layers[ind].bn1.running_var[order_c].data.detach().clone()\n",
    "            net_pruned.module.layers[ind].bn1.running_mean.data = net.module.layers[ind].bn1.running_mean[order_c].data.detach().clone()\n",
    "            order_p = order_c.copy()\n",
    "\n",
    "            n_c = prune_ratio[l]\n",
    "            order_c = np.sort(imp_order[l][n_c:])\n",
    "\n",
    "            net_pruned.module.layers[ind].conv2.weight.data = net.module.layers[ind].conv2.weight[order_c][:,order_p].data.detach().clone()\n",
    "            \n",
    "            net_pruned.module.layers[ind].bn2.weight.data = net.module.layers[ind].bn2.weight[order_c].data.detach().clone()\n",
    "            net_pruned.module.layers[ind].bn2.bias.data = net.module.layers[ind].bn2.bias[order_c].data.detach().clone()\n",
    "            net_pruned.module.layers[ind].bn2.running_var.data = net.module.layers[ind].bn2.running_var[order_c].data.detach().clone()\n",
    "            net_pruned.module.layers[ind].bn2.running_mean.data = net.module.layers[ind].bn2.running_mean[order_c].data.detach().clone()\n",
    "    \n",
    "    n_linear = prune_ratio[-1]\n",
    "    order_linear = np.sort(imp_order[26][n_linear:])\n",
    "\n",
    "    net_pruned.module.linear.weight.data = net.module.linear.weight[:,order_linear].detach().clone()\n",
    "    net_pruned.module.linear.bias.data = net.module.linear.bias.detach().clone()\n",
    "    \n",
    "    return net_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_iter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlated network pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_size = cal_size(net_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_corr, prune_ratio = order_and_ratios(imp_order_corr, 0.2)\n",
    "np.array(prune_ratio), orig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_dict = torch.load(PATH_corr)\n",
    "net_corr.load_state_dict(net_dict['net'])\n",
    "net_p = pruner(net_corr, order_corr, prune_ratio, orig_size, net_type=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_acc(net_corr.eval()), cal_acc(net_p.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_corr = cal_time(net_corr)\n",
    "t_p = cal_time(net_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*(1 - (t_p / t_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def net_p_train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net_p.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net_p(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "def net_p_test(epoch):\n",
    "    global best_p_acc\n",
    "    global prune_iter\n",
    "    net_p.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net_p(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_p_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net_p': net_p.state_dict(),\n",
    "            'best_p_acc': acc\n",
    "        }\n",
    "        if not os.path.isdir('net_p_checkpoint'):\n",
    "            os.mkdir('net_p_checkpoint')\n",
    "        torch.save(state, './net_p_checkpoint/ckpt'+str(prune_iter)+'.pth')\n",
    "        best_p_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net_p.parameters(), lr=0.000001, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_p_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    net_p_train(epoch)\n",
    "    net_p_test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load correlated pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dict = torch.load('./net_p_checkpoint/ckpt1.pth')\n",
    "net_p.load_state_dict(net_dict['net_p'])\n",
    "best_p_acc = net_dict['best_p_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorrelated network pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_size = cal_size(net_decorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 15,  13,   8,   7,  10,   5,   8,   3,  17,  11,  61,  24, 101,\n",
       "         64, 144, 102, 103,  77,  48,  22,  26,  14,  26,  21,  91,  68,\n",
       "          5]),\n",
       " array([  32,   32,   64,   64,  128,  128,  128,  128,  256,  256,  256,\n",
       "         256,  512,  512,  512,  512,  512,  512,  512,  512,  512,  512,\n",
       "         512,  512, 1024, 1024, 1024]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_decorr, prune_ratio = order_and_ratios(imp_order_decorr, 0.1)\n",
    "np.array(prune_ratio), orig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_dict = torch.load(PATH_decorr)\n",
    "net_decorr.load_state_dict(net_dict['net_ortho'])\n",
    "net_p_ortho = pruner(net_decorr, order_decorr, prune_ratio, orig_size, net_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.93\n",
      "44.42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43.93, 44.42)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_acc(net_p_ortho.eval()), cal_acc(net_decorr.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_decorr = cal_time(net_decorr)\n",
    "t_p_ortho = cal_time(net_p_ortho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6359625594480565"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(1 - (t_p_ortho / t_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def net_p_train_ortho(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net_p_ortho.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    angle_cost = 0.0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, labels = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net_p_ortho(inputs)\n",
    "\n",
    "        L_angle = 0\n",
    "\n",
    "        ### Conv_ind == 0 ###\n",
    "        w_mat = net_p_ortho.module.conv1.weight\n",
    "        params = w_mat.reshape(w_mat.shape[0],-1)\n",
    "        angle_mat = torch.matmul(torch.t(params), params) - torch.eye(params.shape[1]).to(device)\n",
    "        L_angle += l_imp['conv1'] * (angle_mat).norm(1) #.norm().pow(2))\n",
    "        \n",
    "        ### Conv_ind != 0 ###        \n",
    "        for lnum in range(13):\n",
    "            w_mat = net_p_ortho.module.layers[lnum].conv1.weight\n",
    "            params = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "            angle_mat = torch.matmul(params.t(), params) - torch.eye(params.shape[1]).to(device)\n",
    "            L_angle += l_imp[lnum] * (angle_mat).norm(1)\n",
    "\n",
    "            w_mat = net_p_ortho.module.layers[lnum].conv2.weight\n",
    "            params = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "            angle_mat = torch.matmul(params.t(), params) - torch.eye(params.shape[1]).to(device)\n",
    "            L_angle += l_imp[lnum] * (angle_mat).norm(1)\n",
    "                \n",
    "        Lc = criterion(outputs, labels)\n",
    "        loss = (1e-1)*(L_angle) + Lc\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        angle_cost += (L_angle).item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (running_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    print(\"angle_cost: \", angle_cost/batch_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_p_test_ortho(epoch):\n",
    "    global best_p_ortho_acc\n",
    "    global prune_iter\n",
    "    net_p_ortho.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net_p_ortho(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    print(acc)\n",
    "    if acc > best_p_ortho_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net_p_ortho': net_p_ortho.state_dict(),\n",
    "            'best_p_ortho_acc': acc\n",
    "        }\n",
    "        if not os.path.isdir('ortho_p_checkpoint'):\n",
    "            os.mkdir('ortho_p_checkpoint')\n",
    "        torch.save(state, './ortho_p_checkpoint/ortho_ckpt'+str(prune_iter)+'.pth')\n",
    "        best_p_ortho_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_imp = {}\n",
    "\n",
    "l_imp.update({'conv1': net_p_ortho.module.conv1.weight.shape[0]})\n",
    "\n",
    "for conv_ind in range(13):\n",
    "    l_imp.update({conv_ind: net_p_ortho.module.layers[conv_ind].conv1.weight.shape[0]})\n",
    "    \n",
    "normalizer = 0\n",
    "for key, val in l_imp.items():\n",
    "    normalizer += val\n",
    "for key, val in l_imp.items():\n",
    "    l_imp[key] = val / normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net_p_ortho.parameters(), lr=0.00001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_p_ortho_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    net_p_train_ortho(epoch)\n",
    "    net_p_test_ortho(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save decorrelated pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dict = torch.load('./ortho_p_checkpoint/ortho_ckpt'+str(prune_iter)+'.pth')\n",
    "net_p_ortho.load_state_dict(net_dict['net_p_ortho'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate orthogonality of filters in pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1255, grad_fn=<DivBackward0>)\n",
      "0 -a:  tensor(0.2985, grad_fn=<DivBackward0>)\n",
      "0 -b:  tensor(0.3180, grad_fn=<DivBackward0>)\n",
      "1 -a:  tensor(0.4291, grad_fn=<DivBackward0>)\n",
      "1 -b:  tensor(0.1325, grad_fn=<DivBackward0>)\n",
      "2 -a:  tensor(0.4088, grad_fn=<DivBackward0>)\n",
      "2 -b:  tensor(0.0655, grad_fn=<DivBackward0>)\n",
      "3 -a:  tensor(0.2347, grad_fn=<DivBackward0>)\n",
      "3 -b:  tensor(0.0719, grad_fn=<DivBackward0>)\n",
      "4 -a:  tensor(0.3798, grad_fn=<DivBackward0>)\n",
      "4 -b:  tensor(0.0404, grad_fn=<DivBackward0>)\n",
      "5 -a:  tensor(0.2231, grad_fn=<DivBackward0>)\n",
      "5 -b:  tensor(0.0571, grad_fn=<DivBackward0>)\n",
      "6 -a:  tensor(0.4774, grad_fn=<DivBackward0>)\n",
      "6 -b:  tensor(0.0313, grad_fn=<DivBackward0>)\n",
      "7 -a:  tensor(0.4358, grad_fn=<DivBackward0>)\n",
      "7 -b:  tensor(0.0336, grad_fn=<DivBackward0>)\n",
      "8 -a:  tensor(0.4617, grad_fn=<DivBackward0>)\n",
      "8 -b:  tensor(0.0288, grad_fn=<DivBackward0>)\n",
      "9 -a:  tensor(0.5335, grad_fn=<DivBackward0>)\n",
      "9 -b:  tensor(0.0249, grad_fn=<DivBackward0>)\n",
      "10 -a:  tensor(0.6149, grad_fn=<DivBackward0>)\n",
      "10 -b:  tensor(0.0249, grad_fn=<DivBackward0>)\n",
      "11 -a:  tensor(0.6082, grad_fn=<DivBackward0>)\n",
      "11 -b:  tensor(0.0222, grad_fn=<DivBackward0>)\n",
      "12 -a:  tensor(0.3951, grad_fn=<DivBackward0>)\n",
      "12 -b:  tensor(0.0117, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_mat = net_p_ortho.module.conv1.weight\n",
    "params = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "angle_mat = torch.matmul(torch.t(params), params) # - torch.eye(params.shape[1]).to(device)\n",
    "L_diag = (angle_mat.diag().norm(1))\n",
    "L_angle = (angle_mat.norm(1))\n",
    "print(L_diag.cpu()/L_angle.cpu())\n",
    "\n",
    "for lnum in range(13):\n",
    "    w_mat = net_p_ortho.module.layers[lnum].conv1.weight\n",
    "    params = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "    angle_mat = torch.matmul(params.t(), params)# - torch.eye(params.shape[0]).to(device)\n",
    "    L_diag = (angle_mat.diag().norm(1))\n",
    "    L_angle = (angle_mat.norm(1))\n",
    "    print(lnum,\"-a: \", L_diag.cpu()/L_angle.cpu())\n",
    "\n",
    "    w_mat = net_p_ortho.module.layers[lnum].conv2.weight\n",
    "    params = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "    angle_mat = torch.matmul(params.t(), params)# - torch.eye(params.shape[0]).to(device)\n",
    "    L_diag = (angle_mat.diag().norm(1))\n",
    "    L_angle = (angle_mat.norm(1))\n",
    "    print(lnum,\"-b: \", L_diag.cpu()/L_angle.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Correlated network '''\n",
    "# with open(\"./w_decorr/pruned_nets/corr/tfo_order/tfo_corr_p\"+str(prune_iter)+\".pkl\", 'rb') as f:\n",
    "#     imp_order_p = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_p.parameters(), lr=0, weight_decay=0)\n",
    "imp_order_p = np.array([[],[],[]]).transpose()\n",
    "i = 0\n",
    "\n",
    "print(i)\n",
    "nlist = cal_importance(net_p, net_p.module.bn1)\n",
    "imp_order_p = np.concatenate((imp_order_p,np.array([np.repeat([i],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "i+=1\n",
    "\n",
    "for l_index in range(13):\n",
    "    print(i)\n",
    "    nlist = cal_importance(net_p, net_p.module.layers[l_index].bn1)\n",
    "    imp_order_p = np.concatenate((imp_order_p,np.array([np.repeat([i],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "    print(i)\n",
    "    nlist = cal_importance(net_p, net_p.module.layers[l_index].bn2)\n",
    "    imp_order_p = np.concatenate((imp_order_p,np.array([np.repeat([i],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "with open(\"./w_decorr/pruned_nets/corr/tfo_order/tfo_corr_p\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "    pickle.dump(imp_order_p, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' De-Correlated network '''\n",
    "with open(\"./w_decorr/pruned_nets/decorr/tfo_order/tfo_w_decorr_p\"+str(prune_iter)+\".pkl\", 'rb') as f:\n",
    "    imp_order_p_ortho = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e0e4c1361f1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mnlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_p_ortho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_p_ortho\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mimp_order_p_ortho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimp_order_p_ortho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ed35fb13ee82>\u001b[0m in \u001b[0;36mcal_importance\u001b[0;34m(net, l_id)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimp_corr_bn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sc/libraries/torch_loc/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sc/libraries/torch_loc/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_std_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(net_p_ortho.parameters(), lr=0, weight_decay=0)\n",
    "imp_order_p_ortho = np.array([[],[],[]]).transpose()\n",
    "i = 0\n",
    "\n",
    "print(i)\n",
    "nlist = cal_importance(net_p_ortho, net_p_ortho.module.bn1)\n",
    "imp_order_p_ortho = np.concatenate((imp_order_p_ortho,np.array([np.repeat([i],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "i+=1\n",
    "\n",
    "for l_index in range(13):\n",
    "    print(i)\n",
    "    nlist = cal_importance(net_p_ortho, net_p_ortho.module.layers[l_index].bn1)\n",
    "    imp_order_p_ortho = np.concatenate((imp_order_p_ortho,np.array([np.repeat([i],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "    print(i)\n",
    "    nlist = cal_importance(net_p_ortho, net_p_ortho.module.layers[l_index].bn2)\n",
    "    imp_order_p_ortho = np.concatenate((imp_order_p_ortho,np.array([np.repeat([i],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "with open(\"./w_decorr/pruned_nets/decorr/tfo_order/tfo_w_decorr_p\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "    pickle.dump(imp_order_p_ortho, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruned network pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Correlated network '''\n",
    "# orig_size = cal_size(net_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' De-Correlated network '''\n",
    "orig_size = cal_size(net_p_ortho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruning order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Correlated network '''\n",
    "# order_p, prune_ratio = order_and_ratios(imp_order_p, 0.1)\n",
    "# np.array(prune_ratio), orig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 15,  13,   8,   7,  11,   5,   8,   4,  17,  10,  59,  24,  98,\n",
       "         62, 143, 105, 103,  78,  48,  27,  26,  12,  28,  21,  88,  69,\n",
       "          5]),\n",
       " array([  17,   17,   56,   56,  118,  118,  120,  120,  239,  239,  195,\n",
       "         195,  411,  411,  368,  368,  409,  409,  464,  464,  486,  486,\n",
       "         486,  486,  933,  933, 1019]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' De-Correlated network '''\n",
    "order_p, prune_ratio = order_and_ratios(imp_order_p_ortho, 0.1)\n",
    "np.array(prune_ratio), orig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_iter = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ''' Correlated network pruning '''\n",
    "net_p1 = pruner(net_p, order_p, prune_ratio, orig_size, net_type=1)\n",
    "\n",
    "print(\"Accs:\", cal_acc(net_p1.eval()), cal_acc(net_p.eval()))\n",
    "print(\"Time:\", cal_time(net_p1), t_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-0a905f878564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m''' De-Correlated network pruning '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet_p1_ortho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_p_ortho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcal_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_p1_ortho\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcal_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_p_ortho\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcal_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_p1_ortho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_decorr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-c343f0ca5881>\u001b[0m in \u001b[0;36mpruner\u001b[0;34m(net, imp_order, prune_ratio, orig_size, net_type)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mnet_pruned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mnet_pruned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mnet_pruned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mnet_pruned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "''' De-Correlated network pruning '''\n",
    "net_p1_ortho = pruner(net_p_ortho, order_p, prune_ratio, orig_size, net_type=2)\n",
    "\n",
    "print(\"Accs:\", cal_acc(net_p1_ortho.eval()), cal_acc(net_p_ortho.eval()))\n",
    "print(\"Time:\", cal_time(net_p1_ortho), t_decorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Correlated network saving '''\n",
    "net_p = net_p1\n",
    "\n",
    "print('Saving..')\n",
    "state = {\n",
    "    'net_p': net_p.state_dict(),\n",
    "    'best_p_acc': cal_acc(net_p.eval())\n",
    "}\n",
    "if not os.path.isdir('net_p_checkpoint'):\n",
    "    os.mkdir('net_p_checkpoint')\n",
    "torch.save(state, './net_p_checkpoint/ckpt'+str(prune_iter)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' De-Correlated network saving '''\n",
    "net_p_ortho = net_p1_ortho\n",
    "\n",
    "print('Saving..')\n",
    "state = {\n",
    "    'net_p_ortho': net_p_ortho.state_dict(),\n",
    "    'best_p_ortho_acc': cal_acc(net_p_ortho.eval())\n",
    "}\n",
    "if not os.path.isdir('ortho_p_checkpoint'):\n",
    "    os.mkdir('ortho_p_checkpoint')\n",
    "torch.save(state, './ortho_p_checkpoint/ortho_ckpt'+str(prune_iter)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Correlated network loading '''\n",
    "# with open(\"./w_decorr/pruned_nets/corr/cfgs/net_p_corr_iter\"+str(1)+\".pkl\", 'rb') as f:\n",
    "#     cfg_p1 = pickle.load(f)\n",
    "    \n",
    "# net_p = torch.nn.DataParallel(MobileNet_p(cfg_p1[0], cfg_p1[1:]))\n",
    "# PATH = './net_p_checkpoint/ckpt'+str(1)+'.pth'\n",
    "# net_p.load_state_dict(torch.load(PATH)['net_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal_acc(net_p.eval()), cal_acc(net_corr.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' De-Correlated network loading '''\n",
    "with open(\"./w_decorr/pruned_nets/decorr/cfgs/net_p_decorr_iter\"+str(1)+\".pkl\", 'rb') as f:\n",
    "    cfg_p1 = pickle.load(f)\n",
    "\n",
    "net_p_ortho = torch.nn.DataParallel(MobileNet_p(cfg_p1[0], cfg_p1[1:]))\n",
    "PATH = './ortho_p_checkpoint/ortho_ckpt'+str(1)+'.pth'\n",
    "net_p_ortho.load_state_dict(torch.load(PATH)['net_p_ortho'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_acc(net_p_ortho.eval()), cal_acc(net_decorr.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLOPS calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.cuda.device(0):\n",
    "    flops, params = get_model_complexity_info(net_p_ortho, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', flops))\n",
    "    \n",
    "# with torch.cuda.device(0):\n",
    "#     flops, params = get_model_complexity_info(net_p, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
    "#     print('{:<30}  {:<8}'.format('Computational complexity: ', flops))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.cuda.device(0):\n",
    "    flops, params = get_model_complexity_info(net_decorr, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', flops))\n",
    "\n",
    "# with torch.cuda.device(0):\n",
    "#     flops, params = get_model_complexity_info(net_corr, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
    "#     print('{:<30}  {:<8}'.format('Computational complexity: ', flops))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
