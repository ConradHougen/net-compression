{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_corr = 0.0002101660919189453\n",
    "t_decorr = 0.0002142816114425659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from imp_baselines import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptflops import get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "#classes = ('plane', 'car', 'bird', 'cat',\n",
    "#          'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''VGG11/13/16/19 in Pytorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "#         if(cfg[vgg_name] == 'VGG13_p'):\n",
    "#             self.classifier = nn.Linear(256, 100)\n",
    "#         else:\n",
    "        self.classifier = nn.Linear(512, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.ReLU(inplace=True),\n",
    "                           nn.BatchNorm2d(x)]\n",
    "                in_channels = x\n",
    "        #layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = VGG('VGG11')\n",
    "    x = torch.randn(2,3,32,32)\n",
    "    y = net(x)\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_p(nn.Module):\n",
    "    def __init__(self, vgg_name, cfg):\n",
    "        super(VGG_p, self).__init__()\n",
    "        self.features = self._make_layers(cfg)\n",
    "        self.classifier = nn.Linear(cfg[-2], 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.ReLU(inplace=True),\n",
    "                           nn.BatchNorm2d(x)]\n",
    "                in_channels = x\n",
    "        #layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = VGG('VGG11')\n",
    "    x = torch.randn(2,3,32,32)\n",
    "    y = net(x)\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_corr = VGG('VGG13').to(device)\n",
    "net_decorr = VGG('VGG13').to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_corr = './cifar100_net.pth'\n",
    "PATH_decorr = './w_decorr/base_params/wnet_base_2.pth'\n",
    "# PATH_decorr = './tempnet1.pth'\n",
    "\n",
    "net_corr.load_state_dict(torch.load(PATH_corr))\n",
    "net_decorr.load_state_dict(torch.load(PATH_decorr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(net_acc):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net_acc(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_importance(net, l_index):\n",
    "    bias_base = net.features[l_index].bias.data.clone().detach()\n",
    "    av_corrval = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    imp_corr_bn = torch.zeros(bias_base.shape[0]).to(device)\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        imp_corr_bn += (((net.features[l_index].weight.grad)*(net.features[l_index].weight.data)) + ((net.features[l_index].bias.grad)*(net.features[l_index].bias.data))).abs().pow(2)\n",
    "\n",
    "    imp_norm = imp_corr_bn\n",
    "    \n",
    "    neuron_order = [np.linspace(0, imp_norm.shape[0]-1, imp_norm.shape[0]), imp_norm]\n",
    "    \n",
    "    return neuron_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_time(net_acc):\n",
    "    testsamp = torch.rand(1,3,32,32).to(device)\n",
    "    \n",
    "    for i in range(5):\n",
    "        net_acc(testsamp)    \n",
    "    \n",
    "    t_s = time.time()\n",
    "    for i in range(5):\n",
    "        net_acc(testsamp)\n",
    "        t_end += time.time() - t_s\n",
    "    \n",
    "    return (t_end / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mass(net, l_index):\n",
    "    num_iter = 0\n",
    "    r = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            num_iter += 1\n",
    "            if(num_iter == 40):\n",
    "                break\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            L_self = 0.0\n",
    "            L_mat = 0.0\n",
    "\n",
    "            for epoch_num in range(1):\n",
    "                out_features = net.features[0:l_index](inputs)\n",
    "                X_t = out_features.reshape(out_features.shape[0], out_features.shape[1], -1)\n",
    "                X_t = torch.div(X_t, X_t.norm(dim=2).reshape(X_t.shape[0],X_t.shape[1],1) + 1e-10)\n",
    "                cov_mat = torch.matmul(X_t, X_t.permute(0,2,1))\n",
    "                L_mat = cov_mat.norm().pow(2)\n",
    "                \n",
    "                ident = (1 - torch.eye(out_features.shape[1])).to(device)\n",
    "                cov_mat = cov_mat*ident\n",
    "                L_self = cov_mat.norm().pow(2)\n",
    "                \n",
    "                r += 1 - L_self/L_mat\n",
    "\n",
    "            del L_self, L_mat, out_features\n",
    "            torch.cuda.empty_cache()\n",
    "        return r/num_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_base_corr = 0\n",
    "# num_stop = 0\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "#         inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#         outputs = net_corr(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss_base_corr += loss.item()\n",
    "#         num_stop += labels.shape[0]\n",
    "#         if(num_stop > 5000):\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imp_order_ground = {}\n",
    "# for l_index in [2, 5, 9, 12, 16, 19, 23, 26, 30, 33]:\n",
    "#     loss_mat = torch.load(\"./w_decorr/loss_corr_bn_train_\"+str(l_index)+\".pt\")\n",
    "#     imp_order_ground.update({l_index: ((loss_mat - loss_base_corr).abs().sort()[1])})#.sort()[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_base_decorr = 0\n",
    "# num_stop = 0\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "#         inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#         outputs = net_decorr(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss_base_decorr += loss.item()\n",
    "#         num_stop += labels.shape[0]\n",
    "#         if(num_stop > 5000):\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imp_order_ground_decorr = {}\n",
    "# for l_index in [2, 5, 9, 12, 16, 19, 23, 26, 30, 33]:\n",
    "#     loss_mat = torch.load(\"./w_decorr/loss_bn_train_\"+str(l_index)+\".pt\")\n",
    "#     imp_order_ground_decorr.update({l_index: ((loss_mat - loss_base_decorr).abs().sort()[1])})#.sort()[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFO importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./w_decorr/base_params/tfo_corr.pkl\", 'rb') as f:\n",
    "    imp_order_corr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_corr.parameters(), lr=0, weight_decay=0)\n",
    "imp_order_corr = np.array([[],[],[]]).transpose()\n",
    "i = 0\n",
    "for l_index in [2, 5, 9, 12, 16, 19, 23, 26, 30, 33]:\n",
    "    print(l_index)\n",
    "    nlist = cal_importance(net_corr, l_index)\n",
    "    imp_order_corr = np.concatenate((imp_order_corr,np.array([np.repeat([l_index],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "# with open(\"./w_decorr/base_params/tfo_corr.pkl\", 'wb') as f:\n",
    "#     pickle.dump(imp_order_corr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "imp_order_decorr = np.array([[],[],[]]).transpose()\n",
    "i = 0\n",
    "for l_index in [2, 5, 9, 12, 16, 19, 23, 26, 30, 33]:\n",
    "    print(l_index)\n",
    "    nlist = cal_importance(net_decorr, l_index)\n",
    "    imp_order_decorr = np.concatenate((imp_order_decorr,np.array([np.repeat([l_index],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "# with open(\"./w_decorr/base_params/tfo_w_decorr_temp.pkl\", 'wb') as f:\n",
    "#     pickle.dump(imp_order_decorr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./w_decorr/base_params/tfo_w_decorr_temp.pkl\", 'rb') as f:\n",
    "    imp_order_decorr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_and_ratios(imp_order, prune_ratio):\n",
    "    imp_sort = np.argsort(imp_order[:,2])\n",
    "    temp_order = imp_order[imp_sort]\n",
    "\n",
    "    n_prune = int(prune_ratio * imp_order.shape[0])\n",
    "\n",
    "    prune_list = temp_order[0:n_prune]\n",
    "\n",
    "    imp_order_tfo = {}\n",
    "    ratios = []\n",
    "\n",
    "    for l_index in [2, 5, 9, 12, 16, 19, 23, 26, 30, 33]:\n",
    "        nlist = temp_order[(temp_order[:,0] == l_index), 1].astype(int)\n",
    "        imp_order_tfo.update({l_index: nlist})\n",
    "        nlist = np.sort(prune_list[(prune_list[:,0] == l_index), 1].astype(int))\n",
    "        ratios.append(nlist.shape[0])\n",
    "    return imp_order_tfo, ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_corr = imp_order_corr.copy()\n",
    "for l_index in [2, 5, 9, 12, 16, 19, 23, 26, 30, 33]:\n",
    "    temp_corr[temp_corr[:,0]==l_index, 2] = torch.load(\"./w_decorr/loss_mats/corr/\"+str(l_index)+\"/loss_corr_bn_train_\"+str(l_index)+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_decorr = imp_order_decorr.copy()\n",
    "for l_index in [2, 5, 9, 12, 16, 19, 23, 26, 30, 33]:\n",
    "    temp_decorr[temp_decorr[:,0]==l_index, 2] = torch.load(\"./w_decorr/loss_mats/decorr/\"+str(l_index)+\"/loss_decorr_bn_train_\"+str(l_index)+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "s = torch.tensor(imp_order_corr[:,2])\n",
    "order = s.sort()[1]\n",
    "vals_corr = s.sort()[0]\n",
    "plt.plot(vals_corr/vals_corr.max())\n",
    "plt.title(\"Correlated (Taylor FO)\")\n",
    "loss_corr_curve = torch.tensor(temp_corr[order,2]/temp_corr[:,2].max())\n",
    "plt.plot(loss_corr_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(vals_corr / vals_corr.max() - loss_corr_curve).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "s = torch.tensor(imp_order_decorr[:,2])\n",
    "order = s.sort()[1]\n",
    "vals_decorr = s.sort()[0]\n",
    "plt.plot(vals_decorr/vals_decorr.max())\n",
    "plt.title(\"Decorrelated (Taylor FO)\")\n",
    "loss_decorr_curve = torch.tensor(temp_decorr[order,2]/temp_decorr[:,2].max())\n",
    "plt.plot(loss_decorr_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(vals_decorr / vals_decorr.max() - loss_decorr_curve).norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfg_p(prune_ratio, orig_size, save_cfg_corr=0, save_cfg=0):\n",
    "    cfg_list = []\n",
    "\n",
    "    for i in range(4):\n",
    "        cfg_list.append(orig_size[2*i] - prune_ratio[2*i])\n",
    "        cfg_list.append(orig_size[2*i+1] - prune_ratio[2*i+1])\n",
    "        cfg_list.append('M')\n",
    "\n",
    "    cfg_list.append(orig_size[8] - prune_ratio[8])\n",
    "    cfg_list.append(orig_size[9] - prune_ratio[9])\n",
    "    cfg_list.append('M')\n",
    "    \n",
    "    if(save_cfg == 1):\n",
    "        with open(\"./w_decorr/pruned_nets/corr/cfgs/net_p_corr_iter\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(cfg_list, f)\n",
    "\n",
    "    elif(save_cfg == 2):\n",
    "        with open(\"./w_decorr/pruned_nets/decorr/cfgs/net_p_decorr_iter\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "            pickle.dump(cfg_list, f)\n",
    "    \n",
    "    return cfg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruner(net, imp_order, prune_ratio, orig_size, net_type=0):\n",
    "    \n",
    "    if(net_type==1):\n",
    "        cfg = cfg_p(prune_ratio, orig_size, save_cfg=1)\n",
    "    elif(net_type==2):\n",
    "        cfg = cfg_p(prune_ratio, orig_size, save_cfg=2)\n",
    "    else:\n",
    "        cfg = cfg_p(prune_ratio, orig_size)        \n",
    "    \n",
    "    net_pruned = VGG_p('VGG13_p', cfg).to(device)\n",
    "    bn = [2, 5, 9, 12, 16, 19, 23, 26, 30, 33]\n",
    "        \n",
    "    for l in range(len(bn)):\n",
    "        if(l == 0):\n",
    "            n_c = prune_ratio[l]\n",
    "            order_c = np.sort(imp_order[bn[l]][n_c:])\n",
    "            net_pruned.features[bn[l]-2].weight.data = net.features[bn[l]-2].weight[order_c].data.detach().clone()\n",
    "            net_pruned.features[bn[l]-2].bias.data = net.features[bn[l]-2].bias[order_c].data.detach().clone()\n",
    "\n",
    "            net_pruned.features[bn[l]].weight.data = net.features[bn[l]].weight[order_c].data.detach().clone()\n",
    "            net_pruned.features[bn[l]].bias.data = net.features[bn[l]].bias[order_c].data.detach().clone()\n",
    "            net_pruned.features[bn[l]].running_var.data = net.features[bn[l]].running_var[order_c].detach().clone()\n",
    "            net_pruned.features[bn[l]].running_mean.data = net.features[bn[l]].running_mean[order_c].detach().clone()    \n",
    "            continue\n",
    "        \n",
    "        n_p = prune_ratio[l-1]        \n",
    "        n_c = prune_ratio[l]\n",
    "\n",
    "        order_p = np.sort(imp_order[bn[l-1]][n_p:])\n",
    "        order_c = np.sort(imp_order[bn[l]][n_c:])\n",
    "        \n",
    "        net_pruned.features[bn[l]-2].weight.data = net.features[bn[l]-2].weight[order_c][:,order_p].detach().clone()\n",
    "        net_pruned.features[bn[l]-2].bias.data = net.features[bn[l]-2].bias[order_c].detach().clone()\n",
    "\n",
    "        net_pruned.features[bn[l]].weight.data = net.features[bn[l]].weight[order_c].detach().clone()\n",
    "        net_pruned.features[bn[l]].bias.data = net.features[bn[l]].bias[order_c].detach().clone()    \n",
    "        net_pruned.features[bn[l]].running_var.data = net.features[bn[l]].running_var[order_c].detach().clone()\n",
    "        net_pruned.features[bn[l]].running_mean.data = net.features[bn[l]].running_mean[order_c].detach().clone()    \n",
    "    \n",
    "    n_33 = prune_ratio[-1]\n",
    "    order_33 = np.sort(imp_order[33][n_33:])\n",
    "\n",
    "    net_pruned.classifier.weight.data = net.classifier.weight[:,order_33].detach().clone()\n",
    "    net_pruned.classifier.bias.data = net.classifier.bias.detach().clone()\n",
    "    \n",
    "    return net_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_iter = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_imp = []\n",
    "\n",
    "for layer_index in [3, 6, 10, 13, 17, 20, 24, 27, 31, 34]:\n",
    "    \n",
    "    _, _, w_in, h_in = net_corr.features[0:layer_index](torch.zeros(1,3,32,32).to(device)).shape\n",
    "    \n",
    "    c_out, c_in, w_f, h_f = net_corr.features[layer_index-3].weight.shape\n",
    "    \n",
    "    c_imp.append((c_in*w_f*h_f)*(w_in*h_in)*c_out*(c_out*(c_in*w_f*h_f)))\n",
    "    \n",
    "c_imp = np.array(c_imp)\n",
    "c_imp = c_imp/c_imp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlated network pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_size = np.array([net_corr.features[0].weight.shape[0], net_corr.features[3].weight.shape[0], net_corr.features[7].weight.shape[0], net_corr.features[10].weight.shape[0], net_corr.features[14].weight.shape[0], net_corr.features[17].weight.shape[0], net_corr.features[21].weight.shape[0], net_corr.features[24].weight.shape[0], net_corr.features[28].weight.shape[0], net_corr.features[31].weight.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruning order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_order_temp = np.copy(imp_order_corr)\n",
    "i = 0\n",
    "for l_index in [2, 5, 9, 12, 16, 19, 23, 26, 30, 33]:\n",
    "    imp_order_temp[imp_order_corr[:,0] == l_index,2] += (c_imp[i])**(6)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(c_imp)**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order_corr, prune_ratio = order_and_ratios(imp_order_corr, 0.4)\n",
    "order_corr, prune_ratio = order_and_ratios(imp_order_temp, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_ratio, orig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# net_corr.load_state_dict(torch.load(PATH_corr))\n",
    "# net_p = pruner(net_corr, order_corr, prune_ratio, orig_size, net_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cal_acc(net_corr.eval()), cal_acc(net_p.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "for i in range(100):\n",
    "    t += (cal_time(net_p))\n",
    "print(1 - t/(100*t_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load correlated pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PATH = './w_decorr/pruned_nets/corr/nets/net_p_iter'+str(prune_iter)+'.pth'\n",
    "PATH = './w_decorr/pruned_nets/net_temp1.pth'\n",
    "net_p.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net_p.parameters(), lr=0.000001, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-4, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    num_iter = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        num_iter += 1\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net_p(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "          (epoch + 1, i + 1, running_loss / num_iter))\n",
    "    print(cal_acc(net_p.eval()))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save correlated pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './w_decorr/pruned_nets/corr/nets/net_p_iter'+str(prune_iter)+'.pth'\n",
    "# PATH = './w_decorr/pruned_nets/net_temp1.pth'\n",
    "torch.save(net_p.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorrelated network pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_size = np.array([net_decorr.features[0].weight.shape[0], net_decorr.features[3].weight.shape[0], net_decorr.features[7].weight.shape[0], net_decorr.features[10].weight.shape[0], net_decorr.features[14].weight.shape[0], net_decorr.features[17].weight.shape[0], net_decorr.features[21].weight.shape[0], net_decorr.features[24].weight.shape[0], net_decorr.features[28].weight.shape[0], net_decorr.features[31].weight.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruning order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_order_temp = np.copy(imp_order_decorr)\n",
    "i = 0\n",
    "for l_index in [2, 5, 9, 12, 16, 19, 23, 26, 30, 33]:\n",
    "    imp_order_temp[imp_order_decorr[:,0] == l_index,2] += (c_imp[i])**(6)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(c_imp)**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order_decorr, prune_ratio = order_and_ratios(imp_order_decorr, 0.4)\n",
    "order_decorr, prune_ratio = order_and_ratios(imp_order_temp, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_ratio, orig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_decorr.load_state_dict(torch.load(PATH_decorr))\n",
    "net_p = pruner(net_decorr, order_decorr, prune_ratio, orig_size, net_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cal_acc(net_decorr.eval()), cal_acc(net_p.eval()) #, cal_acc(net_p1.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "for i in range(5):\n",
    "    t += (cal_time(net_p))\n",
    "print(1 - t/(5*t_decorr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load decorrelated pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './w_decorr/pruned_nets/decorr/nets/net_p_iter'+str(prune_iter)+'.pth'\n",
    "# # PATH = './w_decorr/pruned_nets/net_temp.pth'\n",
    "# net_p.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_imp_p = []\n",
    "\n",
    "l_inds = [0, 3, 6, 10, 13, 17, 20, 24, 27, 31, 34]\n",
    "out = torch.zeros(1,3,32,32).to(device)\n",
    "for i in range(len(l_inds)-1):\n",
    "    time_init = time.time()\n",
    "    out = net_p.features[l_inds[i]:l_inds[i+1]](out)\n",
    "    l_imp_p.append(time.time() - time_init)\n",
    "    \n",
    "l_imp_p = np.array(l_imp_p)\n",
    "l_imp_p = l_imp_p/l_imp_p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_impd = {}\n",
    "i = 0\n",
    "for conv_ind in [0, 3, 7, 10, 14, 17, 21, 24, 28, 31]:\n",
    "    l_impd.update({conv_ind: l_imp_p[i]})\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_impd[0] = 0 #l_impd[31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load decorrelated pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './w_decorr/pruned_nets/decorr/nets/net_p_iter'+str(prune_iter)+'.pth'\n",
    "# PATH = './w_decorr/pruned_nets/net_temp.pth'\n",
    "net_p.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net_p.parameters(), lr=0.00001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    num_iter = 0\n",
    "    angle_cost = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        num_iter += 1\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net_p(inputs)\n",
    "        L_angle = 0\n",
    "        \n",
    "        ### Conv_ind == 0 ###\n",
    "        w_mat = net_p.features[0].weight\n",
    "        w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "\n",
    "        b_mat = net_p.features[0].bias\n",
    "        b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "\n",
    "        params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "\n",
    "        angle_mat = torch.matmul(torch.t(params), params) - torch.eye(params.shape[1]).to(device)\n",
    "\n",
    "#         L_angle += (l_impd[0])*(angle_mat).norm(1) #.norm().pow(2))\n",
    "        L_angle += (angle_mat).norm(1) #.norm().pow(2))        \n",
    "        \n",
    "        ### Conv_ind != 0 ###\n",
    "        for conv_ind in [3, 7, 10, 14, 17, 21, 24, 28, 31]:\n",
    "            w_mat = net_p.features[conv_ind].weight\n",
    "            w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "            \n",
    "            b_mat = net_p.features[conv_ind].bias\n",
    "            b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "            \n",
    "            params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "            \n",
    "            angle_mat = torch.matmul(params, torch.t(params)) - torch.eye(w_mat.shape[0]).to(device)\n",
    "            \n",
    "#             L_angle += (l_impd[conv_ind])*(angle_mat).norm(1)\n",
    "            L_angle += (angle_mat).norm(1)\n",
    "            \n",
    "        Lc = criterion(outputs, labels)\n",
    "        loss = (1e-1)*(L_angle) + Lc\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        angle_cost += (L_angle).item()\n",
    "    \n",
    "    print(\"angle_cost: \", angle_cost/num_iter)\n",
    "    print(\"diag_mass_ratio: \", (num_iter*(64+128+256+1024)*2)/(L_angle.detach().cpu().numpy()))\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "          (epoch + 1, i + 1, running_loss / num_iter))\n",
    "    running_loss = 0.0\n",
    "    print(cal_acc(net_p.eval()))\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save decorrelated pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_iter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './w_decorr/pruned_nets/decorr/nets/net_p_iter'+str(prune_iter)+'.pth'\n",
    "# PATH = './w_decorr/pruned_nets/net_temp.pth'\n",
    "torch.save(net_p.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate orthogonality of filters in pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ind = 0\n",
    "w_mat = net_p.features[conv_ind].weight\n",
    "w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "\n",
    "b_mat = net_p.features[conv_ind].bias\n",
    "b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "\n",
    "params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "\n",
    "angle_mat = torch.matmul(torch.t(params), params)\n",
    "\n",
    "L_diag = (angle_mat.diag().norm(1))\n",
    "L_angle = (angle_mat.norm(1))\n",
    "\n",
    "print(L_diag.cpu()/L_angle.cpu())\n",
    "    \n",
    "for conv_ind in [3, 7, 10, 14, 17, 21, 24, 28, 31]:\n",
    "    w_mat = net_p.features[conv_ind].weight\n",
    "    w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "\n",
    "    b_mat = net_p.features[conv_ind].bias\n",
    "    b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "\n",
    "    params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "\n",
    "    angle_mat = torch.matmul(params, torch.t(params))\n",
    "\n",
    "    L_diag = (angle_mat.diag().norm(1))\n",
    "    L_angle = (angle_mat.norm(1))\n",
    "    \n",
    "    print(L_diag.cpu()/L_angle.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(net_p.parameters(), lr=0, weight_decay=0)\n",
    "# imp_order_p = np.array([[],[],[]]).transpose()\n",
    "# i = 0\n",
    "# for l_index in [2, 5, 9, 12, 16, 19, 23, 26, 30, 33]:\n",
    "#     print(l_index)\n",
    "#     nlist = cal_importance(net_p, l_index)\n",
    "#     imp_order_p = np.concatenate((imp_order_p,np.array([np.repeat([l_index],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "#     i+=1\n",
    "    \n",
    "# with open(\"./w_decorr/pruned_nets/corr/tfo_order/tfo_corr_p\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "#     pickle.dump(imp_order_tfo_p, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_p.parameters(), lr=0, weight_decay=0)\n",
    "imp_order_p = np.array([[],[],[]]).transpose()\n",
    "i = 0\n",
    "for l_index in [2, 5, 9, 12, 16, 19, 23, 26, 30, 33]:\n",
    "    print(l_index)\n",
    "    nlist = cal_importance(net_p, l_index)\n",
    "    imp_order_p = np.concatenate((imp_order_p,np.array([np.repeat([l_index],nlist[1].shape[0]).tolist(), nlist[0].tolist(), nlist[1].detach().cpu().numpy().tolist()]).transpose()), 0)\n",
    "    i+=1\n",
    "    \n",
    "with open(\"./w_decorr/pruned_nets/decorr/tfo_order/tfo_w_decorr_p\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "    pickle.dump(imp_order_p, f)\n",
    "    \n",
    "# with open(\"./w_decorr/pruned_nets/corr/tfo_order/tfo_corr_p\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "#     pickle.dump(imp_order_p, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### De-Correlated network loading ###\n",
    "# with open(\"./w_decorr/pruned_nets/decorr/tfo_order/tfo_w_decorr_p\"+str(prune_iter)+\".pkl\", 'rb') as f:\n",
    "#     imp_order_tfo_p = pickle.load(f)\n",
    "\n",
    "### Correlated network loading ###\n",
    "# with open(\"./w_decorr/pruned_nets/corr/tfo_order/tfo_corr_p\"+str(prune_iter)+\".pkl\", 'wb') as f:\n",
    "#     imp_order_tfo_p = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_imp(net):\n",
    "    t = []\n",
    "    l_inds = [0, 3, 6, 10, 13, 17, 20, 24, 27, 31, 34]\n",
    "    out = torch.zeros(1,3,32,32).to(device)\n",
    "    for i in range(len(l_inds)-1):\n",
    "        time_init = time.time()\n",
    "        out = net.features[l_inds[i]:l_inds[i+1]](out)\n",
    "        t.append(time.time() - time_init)\n",
    "    return np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_imp = t_imp(net_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    c_imp += t_imp(net_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_imp = c_imp / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruned network pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_size = np.array([net_p.features[0].weight.shape[0], net_p.features[3].weight.shape[0], net_p.features[7].weight.shape[0], net_p.features[10].weight.shape[0], net_p.features[14].weight.shape[0], net_p.features[17].weight.shape[0], net_p.features[21].weight.shape[0], net_p.features[24].weight.shape[0], net_p.features[28].weight.shape[0], net_p.features[31].weight.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruning order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_order_temp = np.copy(imp_order_p)\n",
    "i = 0\n",
    "for l_index in [2, 5, 9, 12, 16, 19, 23, 26, 30, 33]:\n",
    "    imp_order_temp[imp_order_p[:,0] == l_index,2] += (1 - 100*c_imp[i])**(500)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1-c_imp*100)**500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_order_p[:,2].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order_corr, prune_ratio = order_and_ratios(imp_order_corr, 0.4)\n",
    "order_p, prune_ratio = order_and_ratios(imp_order_temp, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_ratio, orig_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pruned network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_p1 = pruner(net_p, order_p, prune_ratio, orig_size, net_type=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(cal_acc(net_p1.eval()), cal_acc(net_p.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "for i in range(5):\n",
    "    t += (cal_time(net_p1))\n",
    "    \n",
    "t_decorr = 0\n",
    "\n",
    "for i in range(5):\n",
    "    t_decorr += (cal_time(net_decorr))\n",
    "\n",
    "print(1 - t/(t_decorr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prune the pruned network again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_iter = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_p = net_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### De-Correlated network saving ###\n",
    "PATH = './w_decorr/pruned_nets/decorr/nets/net_p_iter'+str(prune_iter)+'.pth'\n",
    "torch.save(net_p1.state_dict(), PATH)\n",
    "\n",
    "### Correlated network saving ###\n",
    "# PATH = './w_decorr/pruned_nets/corr/nets/net_p_iter'+str(prune_iter)+'.pth'\n",
    "# torch.save(net_p1.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg_p1 = [1, 1, 'M', 1, 1, 'M', 1, 1, 'M', 1, 1, 'M', 1, 1, 'M']\n",
    "# cfg_p1 = []\n",
    "# for layer_index in [3, 6, 10, 13, 17, 20, 24, 27, 31, 34]:\n",
    "#     cfg_p.append(net_p.features[layer_index-1].weight.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./w_decorr/pruned_nets/decorr/cfgs/net_p_decorr_iter3.pkl\", 'rb') as f:\n",
    "    cfg_p1 = pickle.load(f)\n",
    "    \n",
    "# for i in [0, 1, 3, 4, 6, 7, 9, 10, 12, 13]:\n",
    "#     cfg_p1[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_p1 = [32, 64, 'M', 91, 102, 'M', 127, 132, 'M', 114, 112, 'M', 132, 213, 'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_p = VGG_p('VGG13_p', cfg_p1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './w_decorr/pruned_nets/decorr/nets/net_p_iter'+str(5)+'.pth'\n",
    "PATH = './w_decorr/pruned_nets/decorr/nets/net_p_iter3.pth'\n",
    "net_p.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_acc(net_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_acc(net_p.eval()), cal_acc(net_decorr.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLOPS calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.cuda.device(0):\n",
    "    flops, params = get_model_complexity_info(net_p, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.cuda.device(0):\n",
    "    flops, params = get_model_complexity_info(net_decorr, (3, 32, 32), as_strings=True, print_per_layer_stat=True)\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accs_corr = []\n",
    "for prunemuch in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "    order_corr, prune_ratio = order_and_ratios(imp_order_corr, prunemuch)\n",
    "    net_corr.load_state_dict(torch.load(PATH_corr))\n",
    "    net_p = pruner(net_corr, order_corr, prune_ratio, orig_size)\n",
    "    accs_corr.append(cal_acc(net_p.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accs_decorr = []\n",
    "for prunemuch in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "    order_decorr, prune_ratio = order_and_ratios(imp_order_decorr, prunemuch)\n",
    "    net_decorr.load_state_dict(torch.load(PATH_decorr))\n",
    "    net_p = pruner(net_decorr, order_decorr, prune_ratio, orig_size)\n",
    "    accs_decorr.append(cal_acc(net_p.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accs_corr)\n",
    "plt.plot(accs_decorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [3, 6, 10, 13, 17, 20, 24, 27, 31, 34]:\n",
    "    print((net_p.features[i-1].weight - net_corr.features[i-1].weight).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(imp_order_p[:,2], imp_order_p[:,0])\n",
    "plt.xlim(-0.000002, 0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1 - np.array([37.9, 30.9, 23.2, 13.2, 11.2, 6.5, 5.5, 4.4, 3.9]) / 37.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = np.array([60.55, 60.8, 60.85, 61.14, 61.12, 61.16, 60.97, 60.61, 60.41])\n",
    "speedups = np.array([0, 9, 15, 22, 31, 38, 45, 48, 52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_base = np.array([60.52, 60.59, 60.72, 60.83, 60.74, 61.56, 60.97, 60.61, 60.41])\n",
    "speedups_base = np.array([0, 8, 18, 25, 32, 36, 42, 47, 52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(size, accs, label=\"Pruned network's accuracy\")\n",
    "plt.xlabel(\"Compression ratio\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.hlines(xmin=0,xmax=0.9,y=60.55, color='r',label=\"Baseline accuracy\")\n",
    "plt.legend()\n",
    "# plt.savefig(\"ortho_prune.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(size, speedups, label=\"Pruned network's speedup\")\n",
    "plt.xlabel(\"Compression ratio\")\n",
    "plt.ylabel(\"% Inference time\")\n",
    "plt.hlines(xmin=0,xmax=0.9,y=55, color='r', label=\"Maximum possible speedup\")\n",
    "plt.legend()\n",
    "# plt.savefig(\"ortho_time.png\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
