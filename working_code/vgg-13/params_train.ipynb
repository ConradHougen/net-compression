{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "#classes = ('plane', 'car', 'bird', 'cat',\n",
    "#          'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''VGG11/13/16/19 in Pytorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [32, 32, 'M', 64, 64, 'M', 128, 128, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            elif x == 'D2':\n",
    "                layers += [nn.Dropout(p=0.2)]\n",
    "            elif x == 'D3':\n",
    "                layers += [nn.Dropout(p=0.3)]\n",
    "            elif x == 'D4':\n",
    "                layers += [nn.Dropout(p=0.4)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.ReLU(inplace=True),\n",
    "                           nn.BatchNorm2d(x)]\n",
    "                in_channels = x\n",
    "        #layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = VGG('VGG11')\n",
    "    x = torch.randn(2,3,32,32)\n",
    "    y = net(x)\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VGG('VGG13').to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cal_acc(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %.4f %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cal_acc_train(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 50000 train images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mass(net, l_index):\n",
    "    num_iter = 0\n",
    "    r = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            num_iter += 1\n",
    "            if(num_iter == 40):\n",
    "                break\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            L_self = 0.0\n",
    "            L_mat = 0.0\n",
    "\n",
    "            for epoch_num in range(1):\n",
    "                out_features = net.features[0:l_index](inputs)\n",
    "                X_t = out_features.reshape(out_features.shape[0], out_features.shape[1], -1)\n",
    "                X_t = torch.div(X_t, X_t.norm(dim=2).reshape(X_t.shape[0],X_t.shape[1],1) + 1e-10)\n",
    "                cov_mat = torch.matmul(X_t, X_t.permute(0,2,1))\n",
    "                L_mat = cov_mat.norm().pow(2)\n",
    "                \n",
    "                ident = (1 - torch.eye(out_features.shape[1])).to(device)\n",
    "                cov_mat = cov_mat*ident\n",
    "                L_self = cov_mat.norm().pow(2)\n",
    "                \n",
    "                r += 1 - L_self/L_mat\n",
    "\n",
    "            del L_self, L_mat, out_features\n",
    "            torch.cuda.empty_cache()\n",
    "        return r/num_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_index = 33\n",
    "layer_id = 'bn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlated Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar100_net.pth'\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_base = net.features[l_index].weight.data.clone().detach()\n",
    "bias_base = net.features[l_index].bias.data.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_base_corr = 0\n",
    "num_stop = 0\n",
    "for epoch in range(1):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_base_corr += loss.item()\n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break\n",
    "loss_base_corr = loss_base_corr**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_mat_corr = torch.zeros(weight_base.shape[0])\n",
    "\n",
    "# for n_index in range(weight_base.shape[0]): \n",
    "#     num_stop = 0\n",
    "#     print(n_index)\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     net.features[l_index].weight.data[n_index] = 0 #torch.zeros((weight_base.shape[1],weight_base.shape[2],weight_base.shape[3]))\n",
    "#     net.features[l_index].bias.data[n_index] = 0 #torch.zeros((weight_base.shape[1],weight_base.shape[2],weight_base.shape[3]))\n",
    "    \n",
    "# #     for i, data in enumerate(testloader, 0):\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "#         inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "#         outputs = net(inputs)\n",
    "\n",
    "#         loss = (criterion(outputs, labels))\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#         num_stop += labels.shape[0]\n",
    "#         if(num_stop > 5000):\n",
    "#             break\n",
    "            \n",
    "#     loss_mat_corr[n_index] = running_loss**2\n",
    "    \n",
    "#     net.features[l_index].weight.data = weight_base.clone().detach()\n",
    "#     net.features[l_index].bias.data = bias_base.clone().detach()\n",
    "\n",
    "# # torch.save(loss_mat_corr, './w_decorr/loss_corr_bn_train_'+str(l_index)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(loss_mat_corr, './w_decorr/loss_mats/corr/'+str(l_index)+'/loss_corr_bn_train_'+str(l_index)+'.pt')\n",
    "loss_mat_corr = torch.load('./w_decorr/loss_mats/corr/'+str(l_index)+'/loss_corr_bn_train_'+str(l_index)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0, weight_decay=0)\n",
    "av_corrval = 0\n",
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    num_stop = 0\n",
    "    running_loss = 0.0\n",
    "    imp_corr_conv = torch.zeros(bias_base.shape[0]).to(device)\n",
    "    imp_corr_bn = torch.zeros(bias_base.shape[0]).to(device)\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        imp_corr_bn += (((net.features[l_index].weight.grad)*(net.features[l_index].weight.data)) + ((net.features[l_index].bias.grad)*(net.features[l_index].bias.data))).abs().pow(2)\n",
    "        \n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break\n",
    "         \n",
    "    corrval = (np.corrcoef(imp_corr_bn.cpu().detach().numpy(), (loss_mat_corr - loss_base_corr).abs().cpu().detach().numpy()))\n",
    "    print(\"Correlation at epoch \"+str(epoch)+\": \"+str(corrval[0,1]))\n",
    "    av_corrval += corrval[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorrelated net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './w_decorr/base_params/wnet_base_2.pth'\n",
    "# PATH = './tempnet1.pth'\n",
    "net_decorr = VGG('VGG13').to(device)\n",
    "net_decorr.load_state_dict(torch.load(PATH))\n",
    "net_decorr = net_decorr.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_base = net_decorr.features[l_index].weight.data.clone().detach()\n",
    "bias_base = net_decorr.features[l_index].bias.data.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "num_stop = 0\n",
    "loss_base_decorr = 0\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):        \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net_decorr(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_base_decorr += loss.item()\n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break\n",
    "loss_base_decorr = loss_base_decorr**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "\n",
    "# loss_mat_decorr = torch.zeros(weight_base.shape[0])\n",
    "\n",
    "# for n_index in range(weight_base.shape[0]): \n",
    "#     print(n_index)\n",
    "#     num_stop = 0\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "# #     for i, data in enumerate(testloader, 0):\n",
    "#         inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "#         net_decorr.features[l_index].weight.data[n_index] = 0 #torch.zeros((weight_base.shape[1],weight_base.shape[2],weight_base.shape[3]))\n",
    "#         net_decorr.features[l_index].bias.data[n_index] = 0 #torch.zeros((weight_base.shape[1],weight_base.shape[2],weight_base.shape[3]))\n",
    "#         outputs = net_decorr(inputs)\n",
    "        \n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#         num_stop += labels.shape[0]\n",
    "#         if(num_stop > 5000):\n",
    "#             break\n",
    "            \n",
    "#     loss_mat_decorr[n_index] = running_loss**2\n",
    "    \n",
    "#     net_decorr.features[l_index].weight.data = weight_base.clone().detach()\n",
    "#     net_decorr.features[l_index].bias.data = bias_base.clone().detach()\n",
    "\n",
    "# # torch.save(loss_mat_decorr, './w_decorr/loss_bn_train_'+str(l_index)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(loss_mat_decorr, './w_decorr/loss_mats/decorr/'+str(l_index)+'/loss_decorr_bn_train_'+str(l_index)+'.pt')\n",
    "loss_mat_decorr = torch.load('./w_decorr/loss_mats/decorr/'+str(l_index)+'/loss_decorr_bn_train_'+str(l_index)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net_decorr.parameters(), lr=0, weight_decay=0)\n",
    "av_corrval = 0\n",
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    num_stop = 0\n",
    "    imp_decorr_conv = torch.zeros(bias_base.shape[0]).to(device)\n",
    "    imp_decorr_bn = torch.zeros(bias_base.shape[0]).to(device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "#     for i, data in enumerate(testloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net_decorr(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        num_stop += labels.shape[0]\n",
    "        if(num_stop > 5000):\n",
    "            break\n",
    "        \n",
    "        imp_decorr_bn += (((net_decorr.features[l_index].weight.grad)*(net_decorr.features[l_index].weight.data)) + ((net_decorr.features[l_index].bias.grad)*(net_decorr.features[l_index].bias.data))).abs().pow(2)\n",
    "    \n",
    "    corrval = (np.corrcoef(imp_decorr_bn.cpu().detach().numpy(), (loss_mat_decorr - loss_base_decorr).abs().cpu().detach().numpy()))\n",
    "    print(\"Correlation at epoch \"+str(epoch)+\": \"+str(corrval[0,1]))\n",
    "    av_corrval += corrval[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure(figsize=(20,5))\n",
    "# s = imp_corr_bn.cpu().sort()[0].cpu().numpy()\n",
    "# order = imp_corr_bn.sort()[1].cpu().numpy()\n",
    "# plt.plot(s/s.max(), label=\"Estimated importance\")\n",
    "# plt.title(\"Correlated (Taylor FO) for \"+str(l_index))\n",
    "# loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "# plt.xlabel(\"Neuron index\")\n",
    "# plt.ylabel(\"Normalized importance\")\n",
    "# plt.plot(loss_diff[order]/loss_diff.max(), label=\"Actual importance\")\n",
    "# plt.legend()\n",
    "# # plt.savefig(\"./w_decorr/loss_mats/corr/graphs/\"+str(l_index)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure(figsize=(20,5))\n",
    "\n",
    "# s = imp_decorr_bn.cpu().sort()[0].cpu().numpy()\n",
    "# order = imp_decorr_bn.sort()[1].cpu().numpy()\n",
    "# plt.plot(s/s.max(), label=\"Estimated importance\")\n",
    "# plt.title(\"Decorrelated (Taylor FO) for \"+str(l_index))\n",
    "# loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "# plt.xlabel(\"Neuron index\")\n",
    "# plt.ylabel(\"Normalized importance\")\n",
    "# plt.plot(loss_diff[order]/loss_diff.max(), label=\"Actual importance\")\n",
    "# plt.legend()\n",
    "# # plt.savefig(\"./w_decorr/loss_mats/decorr/graphs/\"+str(l_index)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = imp_decorr_bn.cpu().sort()[0].cpu().numpy()\n",
    "s = s/s.max()\n",
    "order = imp_decorr_bn.sort()[1].cpu().numpy()\n",
    "loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "loss_diff = (loss_diff[order]/loss_diff.max())\n",
    "ortho_rms = ((loss_diff - s)**2).sum()\n",
    "\n",
    "s = imp_corr_bn.cpu().sort()[0].cpu().numpy()\n",
    "s = s/s.max()\n",
    "order = imp_corr_bn.sort()[1].cpu().numpy()\n",
    "loss_diff = (loss_mat_corr - loss_base_corr).abs()\n",
    "loss_diff = (loss_diff[order]/loss_diff.max())\n",
    "\n",
    "base_rms = ((loss_diff - s)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_ortho.append(ortho_rms)\n",
    "rms_base.append(base_rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rms_ortho = np.sqrt(np.array(rms_ortho) / np.array([64, 64, 128, 128, 256, 256, 512, 512, 512, 512]))\n",
    "# rms_base = np.sqrt(np.array(rms_base) / np.array([64, 64, 128, 128, 256, 256, 512, 512, 512, 512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_ortho[4] = 0.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(np.linspace(0,30,10)-0.5, rms_ortho, label=\"Decorrelated network\")\n",
    "plt.bar(np.linspace(0,30,10)+0.5, rms_base, label=\"Correlated network\")\n",
    "plt.xlabel(\"Layer ID\")\n",
    "plt.ylabel(\"RMS\")\n",
    "plt.legend()\n",
    "plt.savefig(\"./w_decorr/loss_mats/rms.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20,5))\n",
    "\n",
    "s = imp_decorr_bn.cpu().sort()[0].cpu().numpy()\n",
    "order = imp_decorr_bn.sort()[1].cpu().numpy()\n",
    "axes[0].plot(s/s.max(), label=\"Estimated importance\")\n",
    "axes[0].set_title(\"Decorrelated Network (layer \"+str(l_index)+\")\")\n",
    "loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "axes[0].set_xlabel(\"Neuron index\")\n",
    "axes[0].set_ylabel(\"Normalized importance\")\n",
    "axes[0].plot(loss_diff[order]/loss_diff.max(), label=\"Actual importance\")\n",
    "axes[0].legend()\n",
    "\n",
    "s = imp_corr_bn.cpu().sort()[0].cpu().numpy()\n",
    "order = imp_corr_bn.sort()[1].cpu().numpy()\n",
    "axes[1].plot(s/s.max(), label=\"Estimated importance\")\n",
    "axes[1].set_title(\"Correlated Network (layer \"+str(l_index)+\")\")\n",
    "loss_diff = (loss_mat_corr - loss_base_corr).abs()\n",
    "axes[1].set_xlabel(\"Neuron index\")\n",
    "axes[1].set_ylabel(\"Normalized importance\")\n",
    "axes[1].plot(loss_diff[order]/loss_diff.max(), label=\"Actual importance\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.savefig(\"./w_decorr/loss_mats/subplots/\"+str(l_index)+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net-Slim Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_corr = net.features[l_index].weight.data.clone()\n",
    "np.corrcoef(scale_corr.cpu().numpy(), (loss_mat_corr - loss_base_corr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_decorr = net_decorr.features[l_index].weight.data.clone().abs()\n",
    "np.corrcoef((scale_decorr).cpu().numpy(), (loss_mat_decorr - loss_base_decorr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 based pruning Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_corr = net.features[l_index - 2].weight.data.clone()\n",
    "w_imp_corr = w_corr.pow(2).sum(dim=(1,2,3)).cpu()\n",
    "np.corrcoef(w_imp_corr.numpy(), (loss_mat_corr - loss_base_corr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_decorr = net_decorr.features[l_index - 2].weight.data.clone()\n",
    "w_imp_decorr = w_decorr.pow(2).sum(dim=(1,2,3)).cpu()\n",
    "w_imp_decorr = (w_imp_decorr - w_imp_decorr.min())\n",
    "w_imp_decorr = w_imp_decorr/w_imp_decorr.max()\n",
    "np.corrcoef(w_imp_decorr.numpy(), (loss_mat_decorr - loss_base_decorr).abs().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance plots Netslim Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "\n",
    "s = scale_corr.cpu().sort()[0].cpu().numpy()\n",
    "order = scale_corr.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Correlated (Net-Slim)\")\n",
    "loss_diff = (loss_mat_corr - loss_base_corr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "\n",
    "s = scale_decorr.cpu().sort()[0].cpu().numpy()\n",
    "order = scale_decorr.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Decorrelated (Net-Slim)\")\n",
    "loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance plots L2 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "s = w_imp_corr.sort()[0].cpu().numpy()\n",
    "order = w_imp_corr.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Correlated (L2)\")\n",
    "loss_diff = (loss_mat_corr - loss_base_corr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,5))\n",
    "s = w_imp_decorr.sort()[0].cpu().numpy()\n",
    "order = w_imp_decorr.sort()[1].cpu().numpy()\n",
    "plt.plot(s/s.max())\n",
    "plt.title(\"Decorrelated (L2)\")\n",
    "loss_diff = (loss_mat_decorr - loss_base_decorr).abs()\n",
    "plt.plot(loss_diff[order]/loss_diff.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inner product training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VGG('VGG13').to(device)\n",
    "# PATH = './cifar100_net.pth'\n",
    "PATH = './w_decorr/base_params/wnet_base.pth'\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# net_d = VGG('VGG13').to(device)\n",
    "# PATH_d = './w_decorr/wnet_all.pth'\n",
    "# net_d.load_state_dict(torch.load(PATH_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_acc(net.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_temp = []\n",
    "\n",
    "for layer_index in [3, 6, 10, 13, 17, 20, 24, 27, 31, 34]:\n",
    "    \n",
    "    _, _, w_in, h_in = net.features[0:layer_index](torch.zeros(1,3,32,32).to(device)).shape\n",
    "    \n",
    "    c_out, c_in, w_f, h_f = net.features[layer_index-3].weight.shape\n",
    "    \n",
    "    l_temp.append((c_in*w_f*h_f)*(w_in*h_in)*c_out*(c_out*c_in*w_f*h_f)**(1/4))\n",
    "    \n",
    "    \n",
    "l_temp = np.array(l_temp)\n",
    "l_temp = l_temp/l_temp.sum()\n",
    "\n",
    "l_imp = {}\n",
    "i = 0\n",
    "for layer_index in [0, 3, 7, 10, 14, 17, 21, 24, 28, 31]:\n",
    "    \n",
    "    l_imp.update({layer_index : l_temp[i]})\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_acc(net.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.00001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "time_init = time.time()\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    num_iter = 0\n",
    "    angle_cost = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        num_iter += 1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        L_angle = 0\n",
    "        \n",
    "        ### Conv_ind == 0 ###\n",
    "        w_mat = net.features[0].weight\n",
    "        w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "\n",
    "        b_mat = net.features[0].bias\n",
    "        b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "\n",
    "        params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "\n",
    "        angle_mat = torch.matmul(torch.t(params), params) - torch.eye(params.shape[1]).to(device)\n",
    "\n",
    "        L_angle += (l_imp[0])*(angle_mat).norm(1) #.norm().pow(2))\n",
    "        \n",
    "        ### Conv_ind != 0 ###\n",
    "        \n",
    "        for conv_ind in [3, 7, 10, 14, 17, 21, 24, 28, 31]:\n",
    "            w_mat = net.features[conv_ind].weight\n",
    "            w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "            \n",
    "            b_mat = net.features[conv_ind].bias\n",
    "            b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "            \n",
    "            params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "            \n",
    "            angle_mat = torch.matmul(params, torch.t(params)) - torch.eye(w_mat.shape[0]).to(device)\n",
    "            \n",
    "            L_angle += (l_imp[conv_ind])*(angle_mat).norm(1) #.norm().pow(2))\n",
    "        \n",
    "        Lc = criterion(outputs, labels)\n",
    "        loss = (1e-1)*(L_angle) + Lc\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        angle_cost += (L_angle).item()\n",
    "    \n",
    "    print(\"angle_cost: \", angle_cost/num_iter)\n",
    "#     print(\"diag_mass_ratio: \", (num_iter*(64+128+256+1024)*2)/(L_angle.detach().cpu().numpy()))\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "          (epoch + 1, i + 1, running_loss / num_iter))\n",
    "    running_loss = 0.0\n",
    "    cal_acc(net.eval())\n",
    "print('Finished Training')\n",
    "print(time.time() - time_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_diag():\n",
    "    \n",
    "    w_mat = net.features[0].weight\n",
    "    w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "\n",
    "    b_mat = net.features[0].bias\n",
    "    b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "\n",
    "    params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "\n",
    "    angle_mat = torch.matmul(torch.t(params), params)\n",
    "\n",
    "    L_diag = (angle_mat.diag().norm(1))\n",
    "    L_angle = (angle_mat.norm(1))\n",
    "\n",
    "    print(L_diag.cpu()/L_angle.cpu())\n",
    "\n",
    "    \n",
    "    for conv_ind in [3, 7, 10, 14, 17, 21, 24, 28, 31]:\n",
    "        w_mat = net.features[conv_ind].weight\n",
    "        w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "\n",
    "        b_mat = net.features[conv_ind].bias\n",
    "        b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "\n",
    "        params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "\n",
    "        angle_mat = torch.matmul(params, torch.t(params))\n",
    "\n",
    "        L_diag = (angle_mat.diag().norm(1))\n",
    "        L_angle = (angle_mat.norm(1))\n",
    "\n",
    "        print(L_diag.cpu()/L_angle.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_diag = 0\n",
    "L_angle = 0\n",
    "for conv_ind in [0, 3, 7, 10, 14, 17, 21, 24, 28, 31]:\n",
    "    w_mat = net.features[conv_ind].weight\n",
    "    w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "\n",
    "    b_mat = net.features[conv_ind].bias\n",
    "    b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "\n",
    "#     g_mat = net.features[conv_ind+2].weight\n",
    "#     g_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "\n",
    "#     be_mat = net.features[conv_ind+2].bias\n",
    "#     be_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "\n",
    "    params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "\n",
    "    angle_mat = torch.matmul(params, torch.t(params))\n",
    "\n",
    "    L_diag = (l_imp[conv_ind])*(angle_mat.diag().norm(1))\n",
    "    L_angle = (l_imp[conv_ind])*(angle_mat.norm(1))\n",
    "    \n",
    "    print(L_diag.cpu()/L_angle.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(w_mat1, w_mat1.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_acc(net.eval()), cal_acc_train(net.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = './w_decorr/base_params/wnet_base_2.pth'\n",
    "# torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer prod reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.00001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    num_iter = 0\n",
    "    angle_cost = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        num_iter += 1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        L_angle = 0\n",
    "        \n",
    "        for conv_ind in [0, 3, 7, 10, 14, 17, 21, 24, 28, 31]:\n",
    "            w_mat = net.features[conv_ind].weight\n",
    "            w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "            \n",
    "            b_mat = net.features[conv_ind].bias\n",
    "            b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "            \n",
    "            params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "            \n",
    "            w_wt = params.reshape(params.shape[0], -1)\n",
    "            angle_mat = (w_wt.sum()**2 - (w_wt.sum(dim=1)**2).sum())\n",
    "            L_angle += angle_mat.abs()\n",
    "        \n",
    "        Lc = criterion(outputs, labels)\n",
    "        loss = (1e-3)*(L_angle) + Lc\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        angle_cost += (L_angle).item()\n",
    "    \n",
    "    print(\"angle_cost: \", angle_cost/num_iter)\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "          (epoch + 1, i + 1, running_loss / num_iter))\n",
    "    running_loss = 0.0\n",
    "    cal_acc(net.eval())\n",
    "print('Finished Training')\n",
    "\n",
    "w_diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_diag():\n",
    "    for conv_ind in [0, 3, 7, 10, 14, 17, 21, 24, 28, 31]:\n",
    "\n",
    "        w_mat = net.features[conv_ind].weight\n",
    "        w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "\n",
    "        b_mat = net.features[conv_ind].bias\n",
    "        b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "\n",
    "        params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "\n",
    "        w_wt = params.reshape(params.shape[0], -1)\n",
    "        L_diag = (w_wt.sum(dim=1)**2).sum().abs()\n",
    "        L_angle = ((w_wt.sum()**2) - (w_wt.sum(dim=1)**2).sum()).abs()\n",
    "\n",
    "        print(L_diag.cpu(), L_angle.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "time_init = time.time()\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    num_iter = 0\n",
    "    angle_cost = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        num_iter += 1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        L_angle = 0\n",
    "        \n",
    "        for conv_ind in [3, 7, 10, 14, 17, 21, 24, 28, 31]:\n",
    "            \n",
    "            w_mat = net.features[conv_ind].weight\n",
    "            w_mat1 = (w_mat.reshape(w_mat.shape[0],-1))\n",
    "\n",
    "            b_mat = net.features[conv_ind].bias\n",
    "            b_mat1 = (b_mat.reshape(b_mat.shape[0],-1))\n",
    "\n",
    "            params = torch.cat((w_mat1, b_mat1), dim=1)\n",
    "\n",
    "            angle_mat = torch.matmul(torch.t(params), params) - torch.eye(params.shape[1]).to(device)\n",
    "\n",
    "            L_angle += (l_imp[conv_ind])*(angle_mat).norm(1) #.norm().pow(2))\n",
    "                    \n",
    "        Lc = criterion(outputs, labels)\n",
    "        loss = (1e-4)*(L_angle) + Lc\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        angle_cost += (L_angle).item()\n",
    "    \n",
    "    print(\"angle_cost: \", angle_cost/num_iter)\n",
    "#     print(\"diag_mass_ratio: \", (num_iter*(64+128+256+1024)*2)/(L_angle.detach().cpu().numpy()))\n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "          (epoch + 1, i + 1, running_loss / num_iter))\n",
    "    running_loss = 0.0\n",
    "    cal_acc(net.eval())\n",
    "    w_diag()\n",
    "print('Finished Training')\n",
    "print(time.time() - time_init)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
